{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ip3szDIm4i81",
        "outputId": "ef9fb772-e03e-4d79-c89c-c4787b363088"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1cIGCfx6CiVgEpq8PyKzmF1LBJiQGkxzc\n",
            "From (redirected): https://drive.google.com/uc?id=1cIGCfx6CiVgEpq8PyKzmF1LBJiQGkxzc&confirm=t&uuid=619f3af0-f77e-46a1-b7de-065d15186bd9\n",
            "To: /content/OCT2017.tar.gz\n",
            "100% 5.79G/5.79G [01:20<00:00, 72.3MB/s]\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1JobiELb-4mO_Gk3NY6eyIz-3oRw3U2zT\n",
            "From (redirected): https://drive.google.com/uc?id=1JobiELb-4mO_Gk3NY6eyIz-3oRw3U2zT&confirm=t&uuid=cc649910-d982-4eff-af58-e336164e22cc\n",
            "To: /content/ChestXRay2017.zip\n",
            "100% 1.24G/1.24G [00:12<00:00, 98.6MB/s]\n"
          ]
        }
      ],
      "source": [
        "!gdown --fuzzy \"https://drive.google.com/file/d/1cIGCfx6CiVgEpq8PyKzmF1LBJiQGkxzc/view?usp=sharing\"\n",
        "!gdown --fuzzy \"https://drive.google.com/file/d/1JobiELb-4mO_Gk3NY6eyIz-3oRw3U2zT/view?usp=sharing\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bDPH8R005Wp5"
      },
      "outputs": [],
      "source": [
        "#Extract zip\n",
        "!tar -xzf \"/content/OCT2017.tar.gz\" -C /content/data/\n",
        "!unzip -q /content/ChestXRay2017.zip -d /content/data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wb9-_f0b6NFW",
        "outputId": "844a2a10-798e-4a0e-8abc-41d4a76e53e1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1F_vX0fmLL0nKlhaQMhJWHGcFu9a3NxEs\n",
            "From (redirected): https://drive.google.com/uc?id=1F_vX0fmLL0nKlhaQMhJWHGcFu9a3NxEs&confirm=t&uuid=5cd6fbef-e6a6-4bc3-9976-90d9ff67ef51\n",
            "To: /content/best_mobilenetv3_student_kd.pth\n",
            "100% 39.0M/39.0M [00:00<00:00, 61.5MB/s]\n"
          ]
        }
      ],
      "source": [
        "!gdown --fuzzy \"https://drive.google.com/file/d/1F_vX0fmLL0nKlhaQMhJWHGcFu9a3NxEs/view?usp=sharing\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i-BXzuK96NOR",
        "outputId": "bbde356d-d0c9-4096-8f54-232687265090"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "üöÄ PHASE 3: CONTINUAL LEARNING WITH LWF\n",
            "======================================================================\n",
            "\n",
            "üìÇ Loading Phase 2 model...\n",
            "   ‚úÖ Phase 2 model loaded\n",
            "   üîç Verification: head_a.3.weight shape = torch.Size([4, 256])\n",
            "\n",
            "üìö Creating teacher model (frozen copy)...\n",
            "   ‚úÖ Teacher model created and frozen\n",
            "\n",
            "üìä Creating Task A (OCT) evaluation splits...\n",
            "   Total Task A samples: 83,484\n",
            "   Classes: ['CNV', 'DME', 'DRUSEN', 'NORMAL']\n",
            "   Class distribution: {0: 37205, 1: 11348, 2: 8616, 3: 26315}\n",
            "   Train: 58,438 | Val: 12,523 | Test: 12,523\n",
            "\n",
            "üß™ Evaluating Task A (OCT) BEFORE adapting for task B\n",
            "   Task A Accuracy: 97.06%\n",
            "   Task A F1: 0.9706\n",
            "\n",
            "üìÇ Creating Task B (Chest X-ray) splits...\n",
            "   Total samples: 5,232\n",
            "   Classes: ['NORMAL', 'PNEUMONIA']\n",
            "   Class distribution: {0: 1349, 1: 3883}\n",
            "   Train: 3,662 | Val: 785 | Test: 785\n",
            "   Class weights: [1.9396186 0.6736571]\n",
            "\n",
            "üéØ Training Task B (Chest X-ray) with LwF (Œ±=2.0, T=2.0)...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 29/29 [01:04<00:00,  2.22s/it, loss=4.5041, ce=0.5927, distill=1.9557]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "   Epoch 1 - Task B Val F1: 0.3764 | Acc: 41.15%\n",
            "   üíæ Best model saved (Val F1: 0.3764)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 29/29 [01:04<00:00,  2.22s/it, loss=3.7362, ce=0.3897, distill=1.6732]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "   Epoch 2 - Task B Val F1: 0.7148 | Acc: 69.68%\n",
            "   üìà Task A Retention: F1=0.8186 (84.34% of baseline)\n",
            "   üíæ Best model saved (Val F1: 0.7148)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 29/29 [01:05<00:00,  2.27s/it, loss=2.9508, ce=0.2848, distill=1.3330]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "   Epoch 3 - Task B Val F1: 0.7791 | Acc: 76.43%\n",
            "   üíæ Best model saved (Val F1: 0.7791)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 29/29 [01:04<00:00,  2.22s/it, loss=3.5989, ce=0.3230, distill=1.6379]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "   Epoch 4 - Task B Val F1: 0.8529 | Acc: 84.46%\n",
            "   üìà Task A Retention: F1=0.6550 (67.48% of baseline)\n",
            "   üíæ Best model saved (Val F1: 0.8529)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 29/29 [01:03<00:00,  2.20s/it, loss=2.0307, ce=0.2445, distill=0.8931]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "   Epoch 5 - Task B Val F1: 0.8555 | Acc: 84.71%\n",
            "   üíæ Best model saved (Val F1: 0.8555)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 6/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 29/29 [01:04<00:00,  2.23s/it, loss=1.9000, ce=0.3133, distill=0.7933]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "   Epoch 6 - Task B Val F1: 0.8509 | Acc: 84.20%\n",
            "   üìà Task A Retention: F1=0.5812 (59.88% of baseline)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 7/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 29/29 [01:04<00:00,  2.22s/it, loss=1.7653, ce=0.2193, distill=0.7730]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "   Epoch 7 - Task B Val F1: 0.8555 | Acc: 84.71%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 8/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 29/29 [01:03<00:00,  2.20s/it, loss=1.5397, ce=0.1981, distill=0.6708]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "   Epoch 8 - Task B Val F1: 0.8929 | Acc: 88.79%\n",
            "   üìà Task A Retention: F1=0.5593 (57.63% of baseline)\n",
            "   üíæ Best model saved (Val F1: 0.8929)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 9/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 29/29 [01:04<00:00,  2.22s/it, loss=1.5463, ce=0.2056, distill=0.6703]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "   Epoch 9 - Task B Val F1: 0.8871 | Acc: 88.15%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 10/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 29/29 [01:04<00:00,  2.22s/it, loss=1.1974, ce=0.1712, distill=0.5131]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "   Epoch 10 - Task B Val F1: 0.9094 | Acc: 90.57%\n",
            "   üìà Task A Retention: F1=0.5596 (57.66% of baseline)\n",
            "   üíæ Best model saved (Val F1: 0.9094)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 11/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 29/29 [01:03<00:00,  2.21s/it, loss=1.2401, ce=0.2168, distill=0.5117]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "   Epoch 11 - Task B Val F1: 0.9024 | Acc: 89.81%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 12/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 29/29 [01:03<00:00,  2.20s/it, loss=0.9857, ce=0.1528, distill=0.4165]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "   Epoch 12 - Task B Val F1: 0.9082 | Acc: 90.45%\n",
            "   üìà Task A Retention: F1=0.5700 (58.72% of baseline)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 13/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 29/29 [01:04<00:00,  2.21s/it, loss=0.7998, ce=0.0897, distill=0.3551]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "   Epoch 13 - Task B Val F1: 0.9141 | Acc: 91.08%\n",
            "   üíæ Best model saved (Val F1: 0.9141)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 14/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 29/29 [01:03<00:00,  2.17s/it, loss=0.9941, ce=0.2416, distill=0.3762]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "   Epoch 14 - Task B Val F1: 0.9176 | Acc: 91.46%\n",
            "   üìà Task A Retention: F1=0.5757 (59.31% of baseline)\n",
            "   üíæ Best model saved (Val F1: 0.9176)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 15/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 29/29 [01:03<00:00,  2.18s/it, loss=0.8091, ce=0.1552, distill=0.3270]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "   Epoch 15 - Task B Val F1: 0.9211 | Acc: 91.85%\n",
            "   üíæ Best model saved (Val F1: 0.9211)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 16/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 29/29 [01:03<00:00,  2.20s/it, loss=0.7195, ce=0.1572, distill=0.2812]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "   Epoch 16 - Task B Val F1: 0.9106 | Acc: 90.70%\n",
            "   üìà Task A Retention: F1=0.5888 (60.66% of baseline)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 17/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 29/29 [01:03<00:00,  2.19s/it, loss=0.6696, ce=0.1610, distill=0.2543]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "   Epoch 17 - Task B Val F1: 0.9258 | Acc: 92.36%\n",
            "   üíæ Best model saved (Val F1: 0.9258)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 18/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 29/29 [01:04<00:00,  2.22s/it, loss=0.6053, ce=0.1552, distill=0.2250]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "   Epoch 18 - Task B Val F1: 0.9189 | Acc: 91.59%\n",
            "   üìà Task A Retention: F1=0.5960 (61.40% of baseline)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 19/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 29/29 [01:04<00:00,  2.22s/it, loss=0.5940, ce=0.1582, distill=0.2179]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "   Epoch 19 - Task B Val F1: 0.9200 | Acc: 91.72%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 20/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 29/29 [01:03<00:00,  2.19s/it, loss=0.6033, ce=0.1690, distill=0.2172]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "   Epoch 20 - Task B Val F1: 0.9284 | Acc: 92.61%\n",
            "   üìà Task A Retention: F1=0.6006 (61.88% of baseline)\n",
            "   üíæ Best model saved (Val F1: 0.9284)\n",
            "\n",
            "======================================================================\n",
            "üìä FINAL EVALUATION\n",
            "======================================================================\n",
            "\n",
            "======================================================================\n",
            "üìä TASK B (Chest X-ray) EVALUATION\n",
            "======================================================================\n",
            "   Accuracy:  94.90%\n",
            "   F1-Score:  0.9500\n",
            "\n",
            "üìã Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      NORMAL     0.8559    0.9655    0.9074       203\n",
            "   PNEUMONIA     0.9874    0.9433    0.9649       582\n",
            "\n",
            "    accuracy                         0.9490       785\n",
            "   macro avg     0.9217    0.9544    0.9361       785\n",
            "weighted avg     0.9534    0.9490    0.9500       785\n",
            "\n",
            "\n",
            "======================================================================\n",
            "üìä TASK A (OCT) - Retention Check EVALUATION\n",
            "======================================================================\n",
            "   Accuracy:  61.31%\n",
            "   F1-Score:  0.6006\n",
            "\n",
            "üìã Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         CNV     0.9370    0.4076    0.5681      5581\n",
            "         DME     0.9586    0.2720    0.4238      1702\n",
            "      DRUSEN     0.2935    0.8036    0.4300      1293\n",
            "      NORMAL     0.6425    0.9883    0.7787      3947\n",
            "\n",
            "    accuracy                         0.6131     12523\n",
            "   macro avg     0.7079    0.6179    0.5501     12523\n",
            "weighted avg     0.7807    0.6131    0.6006     12523\n",
            "\n",
            "\n",
            "======================================================================\n",
            "üéØ CONTINUAL LEARNING SUMMARY\n",
            "======================================================================\n",
            "üìä Task A (OCT) Retention:\n",
            "   Before: F1=0.9706, Acc=97.06%\n",
            "   After:  F1=0.6006, Acc=61.31%\n",
            "   Retention: F1=61.88%, Acc=63.17%\n",
            "\n",
            "üìä Task B (Chest X-ray) Performance:\n",
            "   Test F1: 0.9500\n",
            "   Test Acc: 94.90%\n",
            "======================================================================\n",
            "   ‚úÖ Confusion matrix saved: /content/phase3_lwf_results/cm_task_a.png\n",
            "   ‚úÖ Confusion matrix saved: /content/phase3_lwf_results/cm_task_b.png\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "import torchvision.models as models\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, f1_score, accuracy_score, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from collections import Counter\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "import copy\n",
        "\n",
        "# ============================================================================\n",
        "# CONFIGURATION\n",
        "# ============================================================================\n",
        "class Config:\n",
        "    # Paths\n",
        "    TASK_A_DATA_PATH = \"/content/data/OCT2017/train\"  # OCT images folder\n",
        "    TASK_B_DATA_PATH = \"/content/data/chest_xray/train\"  # Chest X-ray images folder\n",
        "    PHASE2_MODEL_PATH = \"/content/best_mobilenetv3_student_kd.pth\"\n",
        "    SAVE_DIR = \"/content/phase3_lwf_results\"\n",
        "\n",
        "    # Model settings\n",
        "    TASK_A_CLASSES = 4  # OCT classes\n",
        "    TASK_B_CLASSES = 2  # Chest X-ray classes\n",
        "\n",
        "    # LwF hyperparameters\n",
        "    LWF_ALPHA = 2.0  # Distillation loss weight\n",
        "    LWF_TEMPERATURE = 2.0\n",
        "\n",
        "    # Training hyperparameters\n",
        "    BATCH_SIZE = 128\n",
        "    NUM_EPOCHS = 20\n",
        "    PATIENCE = 5  # Early stopping\n",
        "\n",
        "    # Data augmentation\n",
        "    USE_AUGMENTATION = True\n",
        "\n",
        "    # Evaluation\n",
        "    EVAL_TASK_A_EVERY = 2  # Evaluate Task A retention every N epochs\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# ============================================================================\n",
        "# DATA LOADING UTILITIES\n",
        "# ============================================================================\n",
        "def load_task_paths(data_path):\n",
        "    \"\"\"\n",
        "    Universal data loader for both OCT and Chest X-ray\n",
        "    \"\"\"\n",
        "    data_path = Path(data_path)\n",
        "\n",
        "    # Get all class folders\n",
        "    class_names = sorted([d.name for d in data_path.iterdir() if d.is_dir()])\n",
        "\n",
        "    all_paths = []\n",
        "    all_labels = []\n",
        "\n",
        "    for idx, class_name in enumerate(class_names):\n",
        "        class_dir = data_path / class_name\n",
        "        # Support multiple image formats\n",
        "        paths = list(class_dir.glob('*.jpeg')) + \\\n",
        "                list(class_dir.glob('*.jpg')) + \\\n",
        "                list(class_dir.glob('*.png'))\n",
        "\n",
        "        all_paths.extend(paths)\n",
        "        all_labels.extend([idx] * len(paths))\n",
        "\n",
        "    return all_paths, all_labels, class_names\n",
        "\n",
        "# ============================================================================\n",
        "# DATASET CLASS\n",
        "# ============================================================================\n",
        "class ImageDataset(Dataset):\n",
        "    def __init__(self, paths, labels, transform=None):\n",
        "        self.paths = paths\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        from PIL import Image\n",
        "        img = Image.open(self.paths[idx]).convert('RGB')\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        return img, label\n",
        "\n",
        "# ============================================================================\n",
        "# DATA SPLITS\n",
        "# ============================================================================\n",
        "def create_task_a_splits():\n",
        "    \"\"\"Create stratified splits for Task A (OCT)\"\"\"\n",
        "    print(\"\\nüìä Creating Task A (OCT) evaluation splits...\")\n",
        "\n",
        "    all_paths, all_labels, task_a_class_names = load_task_paths(Config.TASK_A_DATA_PATH)\n",
        "    print(f\"   Total Task A samples: {len(all_paths):,}\")\n",
        "    print(f\"   Classes: {task_a_class_names}\")\n",
        "\n",
        "    # Class distribution\n",
        "    class_counts = Counter(all_labels)\n",
        "    print(f\"   Class distribution: {dict(class_counts)}\")\n",
        "\n",
        "    # 70/15/15 split\n",
        "    train_paths, temp_paths, train_labels, temp_labels = train_test_split(\n",
        "        all_paths, all_labels, test_size=0.30, stratify=all_labels, random_state=42\n",
        "    )\n",
        "\n",
        "    val_paths, test_paths, val_labels, test_labels = train_test_split(\n",
        "        temp_paths, temp_labels, test_size=0.50, stratify=temp_labels, random_state=42\n",
        "    )\n",
        "\n",
        "    print(f\"   Train: {len(train_paths):,} | Val: {len(val_paths):,} | Test: {len(test_paths):,}\")\n",
        "\n",
        "    test_transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    test_dataset = ImageDataset(test_paths, test_labels, test_transform)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=Config.BATCH_SIZE,\n",
        "                            shuffle=False, num_workers=2)\n",
        "\n",
        "    return test_loader, task_a_class_names\n",
        "\n",
        "def create_task_b_splits():\n",
        "    \"\"\"Create stratified splits for Task B (Chest X-ray)\"\"\"\n",
        "    print(\"\\nüìÇ Creating Task B (Chest X-ray) splits...\")\n",
        "\n",
        "    all_paths, all_labels, task_b_class_names = load_task_paths(Config.TASK_B_DATA_PATH)\n",
        "    print(f\"   Total samples: {len(all_paths):,}\")\n",
        "    print(f\"   Classes: {task_b_class_names}\")\n",
        "\n",
        "    # Class distribution\n",
        "    class_counts = Counter(all_labels)\n",
        "    print(f\"   Class distribution: {dict(class_counts)}\")\n",
        "\n",
        "    # 70/15/15 stratified split\n",
        "    train_paths, temp_paths, train_labels, temp_labels = train_test_split(\n",
        "        all_paths, all_labels, test_size=0.30, stratify=all_labels, random_state=42\n",
        "    )\n",
        "\n",
        "    val_paths, test_paths, val_labels, test_labels = train_test_split(\n",
        "        temp_paths, temp_labels, test_size=0.50, stratify=temp_labels, random_state=42\n",
        "    )\n",
        "\n",
        "    print(f\"   Train: {len(train_paths):,} | Val: {len(val_paths):,} | Test: {len(test_paths):,}\")\n",
        "\n",
        "    # Compute class weights for imbalanced dataset\n",
        "    train_class_counts = Counter(train_labels)\n",
        "    total_samples = len(train_labels)\n",
        "    class_weights = torch.tensor([\n",
        "        total_samples / (len(train_class_counts) * train_class_counts[i])\n",
        "        for i in range(len(task_b_class_names))\n",
        "    ], dtype=torch.float32).to(Config.device)\n",
        "\n",
        "    print(f\"   Class weights: {class_weights.cpu().numpy()}\")\n",
        "\n",
        "    # Data transforms with augmentation\n",
        "    if Config.USE_AUGMENTATION:\n",
        "        train_transform = transforms.Compose([\n",
        "            transforms.Resize((224, 224)),\n",
        "            transforms.RandomHorizontalFlip(p=0.5),\n",
        "            transforms.RandomRotation(10),\n",
        "            transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "        ])\n",
        "    else:\n",
        "        train_transform = transforms.Compose([\n",
        "            transforms.Resize((224, 224)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "        ])\n",
        "\n",
        "    val_transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    train_dataset = ImageDataset(train_paths, train_labels, train_transform)\n",
        "    val_dataset = ImageDataset(val_paths, val_labels, val_transform)\n",
        "    test_dataset = ImageDataset(test_paths, test_labels, val_transform)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=Config.BATCH_SIZE,\n",
        "                             shuffle=True, num_workers=2)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=Config.BATCH_SIZE,\n",
        "                           shuffle=False, num_workers=2)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=Config.BATCH_SIZE,\n",
        "                            shuffle=False, num_workers=2)\n",
        "\n",
        "    return train_loader, val_loader, test_loader, class_weights, task_b_class_names\n",
        "\n",
        "# ============================================================================\n",
        "# MULTI-HEAD MODEL\n",
        "# ============================================================================\n",
        "class MultiHeadMobileNet(nn.Module):\n",
        "    def __init__(self, num_classes_a, num_classes_b):\n",
        "        super().__init__()\n",
        "        # Load MobileNetV3 exactly like Phase 2\n",
        "        mobilenet = models.mobilenet_v3_large(weights=None)\n",
        "        self.features = mobilenet.features  # Backbone features\n",
        "\n",
        "        # Task A head (OCT) - same structure as Phase 2\n",
        "        self.head_a = nn.Sequential(\n",
        "            nn.Linear(960, 256),\n",
        "            nn.Hardswish(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(256, num_classes_a)\n",
        "        )\n",
        "\n",
        "        # Task B head (Chest X-ray) - new\n",
        "        self.head_b = nn.Sequential(\n",
        "            nn.Linear(960, 256),\n",
        "            nn.Hardswish(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(256, num_classes_b)\n",
        "        )\n",
        "\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.flatten = nn.Flatten()\n",
        "\n",
        "    def forward(self, x, task='b'):\n",
        "        x = self.features(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = self.flatten(x)\n",
        "\n",
        "        if task == 'a':\n",
        "            return self.head_a(x)\n",
        "        elif task == 'b':\n",
        "            return self.head_b(x)\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown task: {task}\")\n",
        "\n",
        "# ============================================================================\n",
        "# LWF DISTILLATION LOSS\n",
        "# ============================================================================\n",
        "def distillation_loss(student_logits, teacher_logits, temperature):\n",
        "    \"\"\"\n",
        "    Compute knowledge distillation loss\n",
        "\n",
        "    Args:\n",
        "        student_logits: Raw logits from student model\n",
        "        teacher_logits: Raw logits from teacher model (frozen)\n",
        "        temperature: Temperature for softening probabilities\n",
        "\n",
        "    Returns:\n",
        "        Distillation loss (KL divergence between soft targets)\n",
        "    \"\"\"\n",
        "    # Soften probabilities with temperature\n",
        "    student_soft = nn.functional.log_softmax(student_logits / temperature, dim=1)\n",
        "    teacher_soft = nn.functional.softmax(teacher_logits / temperature, dim=1)\n",
        "\n",
        "    # KL divergence loss\n",
        "    kl_div = nn.functional.kl_div(\n",
        "        student_soft,\n",
        "        teacher_soft,\n",
        "        reduction='batchmean'\n",
        "    )\n",
        "\n",
        "    # Scale by temperature^2 \n",
        "    return kl_div * (temperature ** 2)\n",
        "\n",
        "# ============================================================================\n",
        "# LOAD PHASE 2 MODEL (SAME AS EWC)\n",
        "# ============================================================================\n",
        "def load_phase2_model():\n",
        "    \"\"\"Load Phase 2 model\"\"\"\n",
        "    print(\"\\nüìÇ Loading Phase 2 model...\")\n",
        "\n",
        "    # For LwF, we load from the main model checkpoint (not Fisher file)\n",
        "    if Path(Config.PHASE2_MODEL_PATH).exists():\n",
        "        checkpoint = torch.load(Config.PHASE2_MODEL_PATH, map_location=Config.device)\n",
        "\n",
        "        # Create multi-head model\n",
        "        model = MultiHeadMobileNet(Config.TASK_A_CLASSES, Config.TASK_B_CLASSES)\n",
        "\n",
        "        # Load weights with correct mapping\n",
        "        model_state = {}\n",
        "\n",
        "        # Handle different checkpoint formats\n",
        "        if 'model_state_dict' in checkpoint:\n",
        "            phase2_state = checkpoint['model_state_dict']\n",
        "        else:\n",
        "            phase2_state = checkpoint\n",
        "\n",
        "        for key, value in phase2_state.items():\n",
        "            if key.startswith('backbone.features'):\n",
        "                # backbone.features.X -> features.X\n",
        "                new_key = key.replace('backbone.', '')\n",
        "                model_state[new_key] = value\n",
        "            elif key.startswith('backbone.classifier'):\n",
        "                # backbone.classifier.X -> head_a.X\n",
        "                new_key = key.replace('backbone.classifier', 'head_a')\n",
        "                model_state[new_key] = value\n",
        "\n",
        "        # Load the mapped weights\n",
        "        model.load_state_dict(model_state, strict=False)\n",
        "        model = model.to(Config.device)\n",
        "        print(\"   ‚úÖ Phase 2 model loaded\")\n",
        "        print(f\"   üîç Verification: head_a.3.weight shape = {model.head_a[3].weight.shape}\")\n",
        "\n",
        "        return model\n",
        "    else:\n",
        "        raise FileNotFoundError(f\"Phase 2 model not found at {Config.PHASE2_MODEL_PATH}\")\n",
        "\n",
        "# ============================================================================\n",
        "# EVALUATION FUNCTIONS\n",
        "# ============================================================================\n",
        "def evaluate_task(model, dataloader, task, class_names):\n",
        "    \"\"\"Evaluate model on a specific task\"\"\"\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in dataloader:\n",
        "            images = images.to(Config.device)\n",
        "            outputs = model(images, task=task)\n",
        "            preds = outputs.argmax(dim=1)\n",
        "\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.numpy())\n",
        "\n",
        "    acc = accuracy_score(all_labels, all_preds)\n",
        "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
        "\n",
        "    return acc, f1, all_preds, all_labels\n",
        "\n",
        "def print_evaluation_report(acc, f1, preds, labels, class_names, task_name):\n",
        "    \"\"\"Print detailed evaluation report\"\"\"\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"üìä {task_name} EVALUATION\")\n",
        "    print(f\"{'='*70}\")\n",
        "    print(f\"   Accuracy:  {acc*100:.2f}%\")\n",
        "    print(f\"   F1-Score:  {f1:.4f}\")\n",
        "    print(f\"\\nüìã Classification Report:\")\n",
        "    print(classification_report(labels, preds, target_names=class_names, digits=4))\n",
        "\n",
        "# ============================================================================\n",
        "# TRAINING FUNCTION WITH LWF\n",
        "# ============================================================================\n",
        "def train_phase3_lwf():\n",
        "    \"\"\"Phase 3: Continual Learning with LwF\"\"\"\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"üöÄ PHASE 3: CONTINUAL LEARNING WITH LWF\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # Create save directory\n",
        "    Path(Config.SAVE_DIR).mkdir(exist_ok=True)\n",
        "\n",
        "    # Load Phase 2 model (student)\n",
        "    student_model = load_phase2_model()\n",
        "\n",
        "    # Create teacher model (frozen copy of Phase 2 model)\n",
        "    print(\"\\nüìö Creating teacher model (frozen copy)...\")\n",
        "    teacher_model = copy.deepcopy(student_model)\n",
        "    teacher_model.eval()  # Set to eval mode\n",
        "\n",
        "    # Freeze all teacher parameters\n",
        "    for param in teacher_model.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    print(\"   Teacher model created and frozen\")\n",
        "\n",
        "\n",
        "\n",
        "    # Create Task A test loader for retention evaluation\n",
        "    task_a_test_loader, task_a_classes = create_task_a_splits()\n",
        "\n",
        "    # Evaluate Task A before fine-tuning (baseline)\n",
        "    print(\"\\nüß™ Evaluating Task A (OCT) BEFORE adapting for task B\")\n",
        "    task_a_acc_before, task_a_f1_before, _, _ = evaluate_task(\n",
        "        student_model, task_a_test_loader, task='a', class_names=task_a_classes\n",
        "    )\n",
        "    print(f\"   Task A Accuracy: {task_a_acc_before*100:.2f}%\")\n",
        "    print(f\"   Task A F1: {task_a_f1_before:.4f}\")\n",
        "\n",
        "    # Create Task B dataloaders\n",
        "    train_loader, val_loader, test_loader, class_weights, task_b_classes = create_task_b_splits()\n",
        "\n",
        "    # Setup training\n",
        "    optimizer = optim.Adam([\n",
        "        {'params': student_model.features.parameters(), 'lr': 1e-5},\n",
        "        {'params': student_model.head_b.parameters(), 'lr': 1e-4}\n",
        "    ])\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5,\n",
        "                                                     patience=3)\n",
        "\n",
        "    # Training loop\n",
        "    best_val_f1 = 0.0\n",
        "    patience_counter = 0\n",
        "    history = {'train_loss': [], 'val_f1': [], 'task_a_f1': []}\n",
        "\n",
        "    print(f\"\\nüéØ Training Task B (Chest X-ray) with LwF (Œ±={Config.LWF_ALPHA}, T={Config.LWF_TEMPERATURE})...\")\n",
        "\n",
        "    for epoch in range(Config.NUM_EPOCHS):\n",
        "        # Training\n",
        "        student_model.train()\n",
        "        student_model.head_a.eval()  # Keep Task A head frozen\n",
        "        train_loss = 0.0\n",
        "\n",
        "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{Config.NUM_EPOCHS}\")\n",
        "        for images, labels in pbar:\n",
        "            images, labels = images.to(Config.device), labels.to(Config.device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Student predictions on Task B (new task)\n",
        "            student_logits_b = student_model(images, task='b')\n",
        "\n",
        "            # Task B classification loss\n",
        "            ce_loss = criterion(student_logits_b, labels)\n",
        "\n",
        "            # Get teacher's predictions on Task A (to preserve old knowledge)\n",
        "            with torch.no_grad():\n",
        "                teacher_logits_a = teacher_model(images, task='a')\n",
        "\n",
        "            # Student's predictions on Task A\n",
        "            student_logits_a = student_model(images, task='a')\n",
        "\n",
        "            # Distillation loss (preserve Task A knowledge)\n",
        "            distill_loss = distillation_loss(\n",
        "                student_logits_a,\n",
        "                teacher_logits_a,\n",
        "                Config.LWF_TEMPERATURE\n",
        "            )\n",
        "\n",
        "            # Total loss\n",
        "            total_loss = ce_loss + Config.LWF_ALPHA * distill_loss\n",
        "\n",
        "            total_loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += total_loss.item()\n",
        "            pbar.set_postfix({\n",
        "                'loss': f'{total_loss.item():.4f}',\n",
        "                'ce': f'{ce_loss.item():.4f}',\n",
        "                'distill': f'{distill_loss.item():.4f}'\n",
        "            })\n",
        "\n",
        "        avg_train_loss = train_loss / len(train_loader)\n",
        "        history['train_loss'].append(avg_train_loss)\n",
        "\n",
        "        # Validation on Task B\n",
        "        val_acc, val_f1, _, _ = evaluate_task(student_model, val_loader, task='b',\n",
        "                                             class_names=task_b_classes)\n",
        "        history['val_f1'].append(val_f1)\n",
        "\n",
        "        print(f\"\\n   Epoch {epoch+1} - Task B Val F1: {val_f1:.4f} | Acc: {val_acc*100:.2f}%\")\n",
        "\n",
        "        # Evaluate Task A retention periodically\n",
        "        if (epoch + 1) % Config.EVAL_TASK_A_EVERY == 0:\n",
        "            task_a_acc, task_a_f1, _, _ = evaluate_task(student_model, task_a_test_loader,\n",
        "                                                        task='a', class_names=task_a_classes)\n",
        "            history['task_a_f1'].append(task_a_f1)\n",
        "            retention = (task_a_f1 / task_a_f1_before) * 100\n",
        "            print(f\"   üìà Task A Retention: F1={task_a_f1:.4f} ({retention:.2f}% of baseline)\")\n",
        "\n",
        "        # Learning rate scheduling\n",
        "        scheduler.step(val_f1)\n",
        "\n",
        "        # Early stopping and checkpointing\n",
        "        if val_f1 > best_val_f1:\n",
        "            best_val_f1 = val_f1\n",
        "            patience_counter = 0\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': student_model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'val_f1': val_f1,\n",
        "                'task_a_f1_before': task_a_f1_before\n",
        "            }, f\"{Config.SAVE_DIR}/phase3_lwf_best.pth\")\n",
        "            print(f\"   üíæ Best model saved (Val F1: {val_f1:.4f})\")\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= Config.PATIENCE:\n",
        "                print(f\"\\n‚è∏Ô∏è  Early stopping triggered (patience={Config.PATIENCE})\")\n",
        "                break\n",
        "\n",
        "    # Final evaluation\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"üìä FINAL EVALUATION\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # Load best model\n",
        "    checkpoint = torch.load(f\"{Config.SAVE_DIR}/phase3_lwf_best.pth\")\n",
        "    student_model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "    # Task B (Chest X-ray) - Test set\n",
        "    task_b_acc, task_b_f1, task_b_preds, task_b_labels = evaluate_task(\n",
        "        student_model, test_loader, task='b', class_names=task_b_classes\n",
        "    )\n",
        "    print_evaluation_report(task_b_acc, task_b_f1, task_b_preds, task_b_labels,\n",
        "                          task_b_classes, \"TASK B (Chest X-ray)\")\n",
        "\n",
        "    # Task A (OCT) - Retention test\n",
        "    task_a_acc_after, task_a_f1_after, task_a_preds, task_a_labels = evaluate_task(\n",
        "        student_model, task_a_test_loader, task='a', class_names=task_a_classes\n",
        "    )\n",
        "    print_evaluation_report(task_a_acc_after, task_a_f1_after, task_a_preds, task_a_labels,\n",
        "                          task_a_classes, \"TASK A (OCT) - Retention Check\")\n",
        "\n",
        "    # Retention metrics\n",
        "    retention_f1 = (task_a_f1_after / task_a_f1_before) * 100\n",
        "    retention_acc = (task_a_acc_after / task_a_acc_before) * 100\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"üéØ CONTINUAL LEARNING SUMMARY\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"üìä Task A (OCT) Retention:\")\n",
        "    print(f\"   Before: F1={task_a_f1_before:.4f}, Acc={task_a_acc_before*100:.2f}%\")\n",
        "    print(f\"   After:  F1={task_a_f1_after:.4f}, Acc={task_a_acc_after*100:.2f}%\")\n",
        "    print(f\"   Retention: F1={retention_f1:.2f}%, Acc={retention_acc:.2f}%\")\n",
        "    print(f\"\\nüìä Task B (Chest X-ray) Performance:\")\n",
        "    print(f\"   Test F1: {task_b_f1:.4f}\")\n",
        "    print(f\"   Test Acc: {task_b_acc*100:.2f}%\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # Save confusion matrices\n",
        "    save_confusion_matrix(task_a_labels, task_a_preds, task_a_classes,\n",
        "                         \"Task A (OCT) - After LwF\", f\"{Config.SAVE_DIR}/cm_task_a.png\")\n",
        "    save_confusion_matrix(task_b_labels, task_b_preds, task_b_classes,\n",
        "                         \"Task B (Chest X-ray)\", f\"{Config.SAVE_DIR}/cm_task_b.png\")\n",
        "\n",
        "    return student_model, history\n",
        "\n",
        "def save_confusion_matrix(labels, preds, class_names, title, save_path):\n",
        "    \"\"\"Save confusion matrix plot\"\"\"\n",
        "    cm = confusion_matrix(labels, preds)\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=class_names, yticklabels=class_names)\n",
        "    plt.title(title)\n",
        "    plt.ylabel('True Label')\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "    print(f\"   Confusion matrix saved: {save_path}\")\n",
        "\n",
        "# ============================================================================\n",
        "# MAIN EXECUTION\n",
        "# ============================================================================\n",
        "if __name__ == \"__main__\":\n",
        "    model, history = train_phase3_lwf()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hVJ0ReF5Tynr"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a09bhnx2TzTP"
      },
      "source": [
        "Task A retention dropped to 62.25% (97.29%‚Üí61.63%) despite LwF's knowledge distillation mechanism with Œ±=2.0 and T=2.0. Similar to EWC, this catastrophic forgetting occurred due to BatchNorm statistics drift during Task B training. While LwF successfully preserved the model's prediction patterns through distillation loss (evidenced by low distillation loss values during training), the shifted normalization statistics caused Task A images to be incorrectly scaled at inference time. The forgetting pattern was nearly identical to EWC: minority classes suffered most severely (DME recall: 97%‚Üí25%, CNV: 95%‚Üí43%) while the model became biased towards the NORMAL class (98% recall). This demonstrates that knowledge distillation alone cannot prevent distribution-level forgetting, and both weight-based (EWC) and output-based (LwF) continual learning methods require architectural modifications to address BatchNorm drift in cross-domain scenarios."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2yvKAFR6OpJL"
      },
      "source": [
        "# **Using BatchNorm Freezed**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zQGA4HmOOpTQ",
        "outputId": "59abfc6e-3020-4eec-ce44-bc898cec706c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "üöÄ PHASE 3: CONTINUAL LEARNING WITH LWF\n",
            "======================================================================\n",
            "\n",
            "üìÇ Loading Phase 2 model...\n",
            "   ‚úÖ Phase 2 model loaded\n",
            "   üîç Verification: head_a.3.weight shape = torch.Size([4, 256])\n",
            "\n",
            "üìö Creating teacher model (frozen copy)...\n",
            "   ‚úÖ Teacher model created and frozen\n",
            "\n",
            "üîí Freezing Task A head in student model...\n",
            "   ‚úÖ Task A head frozen\n",
            "\n",
            "üìä Creating Task A (OCT) evaluation splits...\n",
            "   Total Task A samples: 83,484\n",
            "   Classes: ['CNV', 'DME', 'DRUSEN', 'NORMAL']\n",
            "   Class distribution: {0: 37205, 1: 11348, 2: 8616, 3: 26315}\n",
            "   Train: 58,438 | Val: 12,523 | Test: 12,523\n",
            "\n",
            "üß™ Evaluating Task A (OCT) BEFORE adapting for task B\n",
            "   Task A Accuracy: 97.29%\n",
            "   Task A F1: 0.9730\n",
            "\n",
            "üìÇ Creating Task B (Chest X-ray) splits...\n",
            "   Total samples: 5,232\n",
            "   Classes: ['NORMAL', 'PNEUMONIA']\n",
            "   Class distribution: {0: 1349, 1: 3883}\n",
            "   Train: 3,662 | Val: 785 | Test: 785\n",
            "   Class weights: [1.9396186 0.6736571]\n",
            "\n",
            "üéØ Training Task B (Chest X-ray) with LwF (Œ±=2.0, T=2.0)...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/30: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 29/29 [01:01<00:00,  2.13s/it, loss=0.4316, ce=0.4228, distill=0.0044]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "   Epoch 1 - Task B Val F1: 0.8944 | Acc: 89.04%\n",
            "   üíæ Best model saved (Val F1: 0.8944)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/30: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 29/29 [01:02<00:00,  2.16s/it, loss=0.2819, ce=0.2616, distill=0.0101]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "   Epoch 2 - Task B Val F1: 0.9088 | Acc: 90.57%\n",
            "   üìà Task A Retention: F1=0.9740 (100.10% of baseline)\n",
            "   üíæ Best model saved (Val F1: 0.9088)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3/30: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 29/29 [01:01<00:00,  2.13s/it, loss=0.2652, ce=0.2468, distill=0.0092]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "   Epoch 3 - Task B Val F1: 0.9253 | Acc: 92.36%\n",
            "   üíæ Best model saved (Val F1: 0.9253)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4/30: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 29/29 [01:01<00:00,  2.14s/it, loss=0.3476, ce=0.3295, distill=0.0091]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "   Epoch 4 - Task B Val F1: 0.9242 | Acc: 92.23%\n",
            "   üìà Task A Retention: F1=0.9739 (100.09% of baseline)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5/30: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 29/29 [01:02<00:00,  2.14s/it, loss=0.2468, ce=0.2261, distill=0.0104]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "   Epoch 5 - Task B Val F1: 0.9092 | Acc: 90.57%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 6/30: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 29/29 [01:01<00:00,  2.12s/it, loss=0.2248, ce=0.2076, distill=0.0086]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "   Epoch 6 - Task B Val F1: 0.9400 | Acc: 93.89%\n",
            "   üìà Task A Retention: F1=0.9733 (100.04% of baseline)\n",
            "   üíæ Best model saved (Val F1: 0.9400)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 7/30: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 29/29 [01:02<00:00,  2.14s/it, loss=0.3250, ce=0.3046, distill=0.0102]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "   Epoch 7 - Task B Val F1: 0.9400 | Acc: 93.89%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 8/30: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 29/29 [01:02<00:00,  2.15s/it, loss=0.2245, ce=0.1966, distill=0.0139]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "   Epoch 8 - Task B Val F1: 0.9365 | Acc: 93.50%\n",
            "   üìà Task A Retention: F1=0.9724 (99.94% of baseline)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 9/30: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 29/29 [01:01<00:00,  2.11s/it, loss=0.2080, ce=0.1876, distill=0.0102]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "   Epoch 9 - Task B Val F1: 0.9413 | Acc: 94.01%\n",
            "   üíæ Best model saved (Val F1: 0.9413)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 10/30: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 29/29 [01:01<00:00,  2.13s/it, loss=0.1650, ce=0.1428, distill=0.0111]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "   Epoch 10 - Task B Val F1: 0.9391 | Acc: 93.76%\n",
            "   üìà Task A Retention: F1=0.9716 (99.85% of baseline)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 11/30: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 29/29 [01:01<00:00,  2.11s/it, loss=0.2320, ce=0.2108, distill=0.0106]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "   Epoch 11 - Task B Val F1: 0.9271 | Acc: 92.48%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 12/30: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 29/29 [01:01<00:00,  2.13s/it, loss=0.1662, ce=0.1421, distill=0.0120]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "   Epoch 12 - Task B Val F1: 0.9485 | Acc: 94.78%\n",
            "   üìà Task A Retention: F1=0.9682 (99.51% of baseline)\n",
            "   üíæ Best model saved (Val F1: 0.9485)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 13/30: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 29/29 [01:01<00:00,  2.13s/it, loss=0.1147, ce=0.1006, distill=0.0070]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "   Epoch 13 - Task B Val F1: 0.9523 | Acc: 95.16%\n",
            "   üíæ Best model saved (Val F1: 0.9523)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 14/30: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 29/29 [01:01<00:00,  2.12s/it, loss=0.1811, ce=0.1629, distill=0.0091]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "   Epoch 14 - Task B Val F1: 0.9522 | Acc: 95.16%\n",
            "   üìà Task A Retention: F1=0.9652 (99.20% of baseline)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 15/30: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 29/29 [01:01<00:00,  2.13s/it, loss=0.1271, ce=0.1102, distill=0.0085]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "   Epoch 15 - Task B Val F1: 0.9451 | Acc: 94.39%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 16/30: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 29/29 [01:01<00:00,  2.12s/it, loss=0.1447, ce=0.1327, distill=0.0060]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "   Epoch 16 - Task B Val F1: 0.9547 | Acc: 95.41%\n",
            "   üìà Task A Retention: F1=0.9641 (99.09% of baseline)\n",
            "   üíæ Best model saved (Val F1: 0.9547)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 17/30: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 29/29 [01:01<00:00,  2.12s/it, loss=0.1334, ce=0.1207, distill=0.0063]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "   Epoch 17 - Task B Val F1: 0.9557 | Acc: 95.54%\n",
            "   üíæ Best model saved (Val F1: 0.9557)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 18/30: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 29/29 [01:01<00:00,  2.13s/it, loss=0.1018, ce=0.0826, distill=0.0096]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "   Epoch 18 - Task B Val F1: 0.9570 | Acc: 95.67%\n",
            "   üìà Task A Retention: F1=0.9622 (98.90% of baseline)\n",
            "   üíæ Best model saved (Val F1: 0.9570)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 19/30: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 29/29 [01:01<00:00,  2.11s/it, loss=0.1352, ce=0.1242, distill=0.0055]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "   Epoch 19 - Task B Val F1: 0.9631 | Acc: 96.31%\n",
            "   üíæ Best model saved (Val F1: 0.9631)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 20/30: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 29/29 [01:01<00:00,  2.12s/it, loss=0.1018, ce=0.0921, distill=0.0048]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "   Epoch 20 - Task B Val F1: 0.9631 | Acc: 96.31%\n",
            "   üìà Task A Retention: F1=0.9620 (98.88% of baseline)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 21/30: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 29/29 [01:02<00:00,  2.15s/it, loss=0.0909, ce=0.0735, distill=0.0087]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "   Epoch 21 - Task B Val F1: 0.9560 | Acc: 95.54%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 22/30: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 29/29 [01:00<00:00,  2.10s/it, loss=0.1112, ce=0.1005, distill=0.0053]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "   Epoch 22 - Task B Val F1: 0.9548 | Acc: 95.41%\n",
            "   üìà Task A Retention: F1=0.9629 (98.96% of baseline)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 23/30: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 29/29 [01:02<00:00,  2.15s/it, loss=0.0951, ce=0.0854, distill=0.0049]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "   Epoch 23 - Task B Val F1: 0.9631 | Acc: 96.31%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 24/30: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 29/29 [01:02<00:00,  2.17s/it, loss=0.0757, ce=0.0674, distill=0.0041]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "   Epoch 24 - Task B Val F1: 0.9657 | Acc: 96.56%\n",
            "   üìà Task A Retention: F1=0.9610 (98.77% of baseline)\n",
            "   üíæ Best model saved (Val F1: 0.9657)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 25/30: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 29/29 [01:01<00:00,  2.11s/it, loss=0.0992, ce=0.0895, distill=0.0049]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "   Epoch 25 - Task B Val F1: 0.9631 | Acc: 96.31%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 26/30: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 29/29 [01:01<00:00,  2.13s/it, loss=0.0932, ce=0.0794, distill=0.0069]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "   Epoch 26 - Task B Val F1: 0.9644 | Acc: 96.43%\n",
            "   üìà Task A Retention: F1=0.9618 (98.85% of baseline)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 27/30: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 29/29 [01:00<00:00,  2.10s/it, loss=0.0748, ce=0.0593, distill=0.0077]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "   Epoch 27 - Task B Val F1: 0.9644 | Acc: 96.43%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 28/30: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 29/29 [01:01<00:00,  2.12s/it, loss=0.0515, ce=0.0424, distill=0.0046]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "   Epoch 28 - Task B Val F1: 0.9644 | Acc: 96.43%\n",
            "   üìà Task A Retention: F1=0.9625 (98.92% of baseline)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 29/30: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 29/29 [01:01<00:00,  2.13s/it, loss=0.0715, ce=0.0633, distill=0.0041]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "   Epoch 29 - Task B Val F1: 0.9644 | Acc: 96.43%\n",
            "\n",
            "‚è∏Ô∏è  Early stopping triggered (patience=5)\n",
            "\n",
            "======================================================================\n",
            "üìä FINAL EVALUATION\n",
            "======================================================================\n",
            "\n",
            "======================================================================\n",
            "üìä TASK B (Chest X-ray) EVALUATION\n",
            "======================================================================\n",
            "   Accuracy:  97.07%\n",
            "   F1-Score:  0.9709\n",
            "\n",
            "üìã Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      NORMAL     0.9206    0.9704    0.9448       203\n",
            "   PNEUMONIA     0.9895    0.9708    0.9801       582\n",
            "\n",
            "    accuracy                         0.9707       785\n",
            "   macro avg     0.9550    0.9706    0.9624       785\n",
            "weighted avg     0.9717    0.9707    0.9709       785\n",
            "\n",
            "\n",
            "======================================================================\n",
            "üìä TASK A (OCT) - Retention Check EVALUATION\n",
            "======================================================================\n",
            "   Accuracy:  96.06%\n",
            "   F1-Score:  0.9610\n",
            "\n",
            "üìã Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         CNV     0.9896    0.9525    0.9707      5581\n",
            "         DME     0.8858    0.9706    0.9263      1702\n",
            "      DRUSEN     0.8980    0.9258    0.9117      1293\n",
            "      NORMAL     0.9777    0.9792    0.9785      3947\n",
            "\n",
            "    accuracy                         0.9606     12523\n",
            "   macro avg     0.9378    0.9570    0.9468     12523\n",
            "weighted avg     0.9623    0.9606    0.9610     12523\n",
            "\n",
            "\n",
            "======================================================================\n",
            "üéØ CONTINUAL LEARNING SUMMARY\n",
            "======================================================================\n",
            "üìä Task A (OCT) Retention:\n",
            "   Before: F1=0.9730, Acc=97.29%\n",
            "   After:  F1=0.9610, Acc=96.06%\n",
            "   Retention: F1=98.77%, Acc=98.74%\n",
            "\n",
            "üìä Task B (Chest X-ray) Performance:\n",
            "   Test F1: 0.9709\n",
            "   Test Acc: 97.07%\n",
            "======================================================================\n",
            "   ‚úÖ Confusion matrix saved: /content/phase3_lwf_results/cm_task_a.png\n",
            "   ‚úÖ Confusion matrix saved: /content/phase3_lwf_results/cm_task_b.png\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "import torchvision.models as models\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, f1_score, accuracy_score, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from collections import Counter\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "import copy\n",
        "\n",
        "# ============================================================================\n",
        "# CONFIGURATION\n",
        "# ============================================================================\n",
        "class Config:\n",
        "    # Paths\n",
        "    TASK_A_DATA_PATH = \"/content/data/OCT2017/train\"  # OCT images folder\n",
        "    TASK_B_DATA_PATH = \"/content/data/chest_xray/train\"  # Chest X-ray images folder\n",
        "    PHASE2_MODEL_PATH = \"/content/best_mobilenetv3_student_kd.pth\"\n",
        "    SAVE_DIR = \"/content/phase3_lwf_results\"\n",
        "\n",
        "    # Model settings\n",
        "    TASK_A_CLASSES = 4  # OCT classes\n",
        "    TASK_B_CLASSES = 2  # Chest X-ray classes\n",
        "\n",
        "    # LwF hyperparameters\n",
        "    LWF_ALPHA = 2.0  # Distillation loss weight \n",
        "    LWF_TEMPERATURE = 2.0\n",
        "\n",
        "    # Training hyperparameters\n",
        "    BATCH_SIZE = 128\n",
        "    NUM_EPOCHS = 30\n",
        "    PATIENCE = 5  # Early stopping\n",
        "\n",
        "    # Data augmentation\n",
        "    USE_AUGMENTATION = True\n",
        "\n",
        "    # Evaluation\n",
        "    EVAL_TASK_A_EVERY = 2  # Evaluate Task A retention every N epochs\n",
        "    FREEZE_BATCHNORM = True\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# ============================================================================\n",
        "# DATA LOADING UTILITIES \n",
        "# ============================================================================\n",
        "def load_task_paths(data_path):\n",
        "    \"\"\"\n",
        "    Universal data loader for both OCT and Chest X-ray\n",
        "    Loads from: /path/to/train/CLASS_NAME/*.jpg\n",
        "    \"\"\"\n",
        "    data_path = Path(data_path)\n",
        "\n",
        "    # Get all class folders\n",
        "    class_names = sorted([d.name for d in data_path.iterdir() if d.is_dir()])\n",
        "\n",
        "    all_paths = []\n",
        "    all_labels = []\n",
        "\n",
        "    for idx, class_name in enumerate(class_names):\n",
        "        class_dir = data_path / class_name\n",
        "        # Support multiple image formats\n",
        "        paths = list(class_dir.glob('*.jpeg')) + \\\n",
        "                list(class_dir.glob('*.jpg')) + \\\n",
        "                list(class_dir.glob('*.png'))\n",
        "\n",
        "        all_paths.extend(paths)\n",
        "        all_labels.extend([idx] * len(paths))\n",
        "\n",
        "    return all_paths, all_labels, class_names\n",
        "\n",
        "# ============================================================================\n",
        "# DATASET CLASS \n",
        "# ============================================================================\n",
        "class ImageDataset(Dataset):\n",
        "    def __init__(self, paths, labels, transform=None):\n",
        "        self.paths = paths\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        from PIL import Image\n",
        "        img = Image.open(self.paths[idx]).convert('RGB')\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        return img, label\n",
        "\n",
        "# ============================================================================\n",
        "# DATA SPLITS \n",
        "# ============================================================================\n",
        "def create_task_a_splits():\n",
        "    \"\"\"Create stratified splits for Task A (OCT)\"\"\"\n",
        "    print(\"\\nüìä Creating Task A (OCT) evaluation splits...\")\n",
        "\n",
        "    all_paths, all_labels, task_a_class_names = load_task_paths(Config.TASK_A_DATA_PATH)\n",
        "    print(f\"   Total Task A samples: {len(all_paths):,}\")\n",
        "    print(f\"   Classes: {task_a_class_names}\")\n",
        "\n",
        "    # Class distribution\n",
        "    class_counts = Counter(all_labels)\n",
        "    print(f\"   Class distribution: {dict(class_counts)}\")\n",
        "\n",
        "    # 70/15/15 split\n",
        "    train_paths, temp_paths, train_labels, temp_labels = train_test_split(\n",
        "        all_paths, all_labels, test_size=0.30, stratify=all_labels, random_state=42\n",
        "    )\n",
        "\n",
        "    val_paths, test_paths, val_labels, test_labels = train_test_split(\n",
        "        temp_paths, temp_labels, test_size=0.50, stratify=temp_labels, random_state=42\n",
        "    )\n",
        "\n",
        "    print(f\"   Train: {len(train_paths):,} | Val: {len(val_paths):,} | Test: {len(test_paths):,}\")\n",
        "\n",
        "    test_transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    test_dataset = ImageDataset(test_paths, test_labels, test_transform)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=Config.BATCH_SIZE,\n",
        "                            shuffle=False, num_workers=2)\n",
        "\n",
        "    return test_loader, task_a_class_names\n",
        "\n",
        "def create_task_b_splits():\n",
        "    \"\"\"Create stratified splits for Task B (Chest X-ray)\"\"\"\n",
        "    print(\"\\n Creating Task B (Chest X-ray) splits...\")\n",
        "\n",
        "    all_paths, all_labels, task_b_class_names = load_task_paths(Config.TASK_B_DATA_PATH)\n",
        "    print(f\"   Total samples: {len(all_paths):,}\")\n",
        "    print(f\"   Classes: {task_b_class_names}\")\n",
        "\n",
        "    # Class distribution\n",
        "    class_counts = Counter(all_labels)\n",
        "    print(f\"   Class distribution: {dict(class_counts)}\")\n",
        "\n",
        "    # 70/15/15 stratified split\n",
        "    train_paths, temp_paths, train_labels, temp_labels = train_test_split(\n",
        "        all_paths, all_labels, test_size=0.30, stratify=all_labels, random_state=42\n",
        "    )\n",
        "\n",
        "    val_paths, test_paths, val_labels, test_labels = train_test_split(\n",
        "        temp_paths, temp_labels, test_size=0.50, stratify=temp_labels, random_state=42\n",
        "    )\n",
        "\n",
        "    print(f\"   Train: {len(train_paths):,} | Val: {len(val_paths):,} | Test: {len(test_paths):,}\")\n",
        "\n",
        "    # Compute class weights for imbalanced dataset\n",
        "    train_class_counts = Counter(train_labels)\n",
        "    total_samples = len(train_labels)\n",
        "    class_weights = torch.tensor([\n",
        "        total_samples / (len(train_class_counts) * train_class_counts[i])\n",
        "        for i in range(len(task_b_class_names))\n",
        "    ], dtype=torch.float32).to(Config.device)\n",
        "\n",
        "    print(f\"   Class weights: {class_weights.cpu().numpy()}\")\n",
        "\n",
        "    # Data transforms with augmentation\n",
        "    if Config.USE_AUGMENTATION:\n",
        "        train_transform = transforms.Compose([\n",
        "            transforms.Resize((224, 224)),\n",
        "            transforms.RandomHorizontalFlip(p=0.5),\n",
        "            transforms.RandomRotation(10),\n",
        "            transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "        ])\n",
        "    else:\n",
        "        train_transform = transforms.Compose([\n",
        "            transforms.Resize((224, 224)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "        ])\n",
        "\n",
        "    val_transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    train_dataset = ImageDataset(train_paths, train_labels, train_transform)\n",
        "    val_dataset = ImageDataset(val_paths, val_labels, val_transform)\n",
        "    test_dataset = ImageDataset(test_paths, test_labels, val_transform)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=Config.BATCH_SIZE,\n",
        "                             shuffle=True, num_workers=2)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=Config.BATCH_SIZE,\n",
        "                           shuffle=False, num_workers=2)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=Config.BATCH_SIZE,\n",
        "                            shuffle=False, num_workers=2)\n",
        "\n",
        "    return train_loader, val_loader, test_loader, class_weights, task_b_class_names\n",
        "\n",
        "# ============================================================================\n",
        "# MULTI-HEAD MODEL\n",
        "# ============================================================================\n",
        "class MultiHeadMobileNet(nn.Module):\n",
        "    def __init__(self, num_classes_a, num_classes_b):\n",
        "        super().__init__()\n",
        "        \n",
        "        mobilenet = models.mobilenet_v3_large(weights=None)\n",
        "        self.features = mobilenet.features  # Backbone features\n",
        "\n",
        "        # Task A head (OCT) - same structure as Phase 2\n",
        "        self.head_a = nn.Sequential(\n",
        "            nn.Linear(960, 256),\n",
        "            nn.Hardswish(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(256, num_classes_a)\n",
        "        )\n",
        "\n",
        "        # Task B head (Chest X-ray) - new\n",
        "        self.head_b = nn.Sequential(\n",
        "            nn.Linear(960, 256),\n",
        "            nn.Hardswish(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(256, num_classes_b)\n",
        "        )\n",
        "\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.flatten = nn.Flatten()\n",
        "\n",
        "    def forward(self, x, task='b'):\n",
        "        x = self.features(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = self.flatten(x)\n",
        "\n",
        "        if task == 'a':\n",
        "            return self.head_a(x)\n",
        "        elif task == 'b':\n",
        "            return self.head_b(x)\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown task: {task}\")\n",
        "\n",
        "# ============================================================================\n",
        "# LWF DISTILLATION LOSS\n",
        "# ============================================================================\n",
        "def distillation_loss(student_logits, teacher_logits, temperature):\n",
        "    \"\"\"\n",
        "    Compute knowledge distillation loss\n",
        "\n",
        "    Args:\n",
        "        student_logits: Raw logits from student model\n",
        "        teacher_logits: Raw logits from teacher model (frozen)\n",
        "        temperature: Temperature for softening probabilities\n",
        "\n",
        "    Returns:\n",
        "        Distillation loss (KL divergence between soft targets)\n",
        "    \"\"\"\n",
        "    # Soften probabilities with temperature\n",
        "    student_soft = nn.functional.log_softmax(student_logits / temperature, dim=1)\n",
        "    teacher_soft = nn.functional.softmax(teacher_logits / temperature, dim=1)\n",
        "\n",
        "    # KL divergence loss\n",
        "    kl_div = nn.functional.kl_div(\n",
        "        student_soft,\n",
        "        teacher_soft,\n",
        "        reduction='batchmean'\n",
        "    )\n",
        "\n",
        "    # Scale by temperature^2 \n",
        "    return kl_div * (temperature ** 2)\n",
        "\n",
        "# ============================================================================\n",
        "# LOAD PHASE 2 MODEL \n",
        "# ============================================================================\n",
        "def load_phase2_model():\n",
        "    \"\"\"Load Phase 2 model\"\"\"\n",
        "    print(\"\\n Loading Phase 2 model...\")\n",
        "\n",
        "    \n",
        "    if Path(Config.PHASE2_MODEL_PATH).exists():\n",
        "        checkpoint = torch.load(Config.PHASE2_MODEL_PATH, map_location=Config.device)\n",
        "\n",
        "        # Create multi-head model\n",
        "        model = MultiHeadMobileNet(Config.TASK_A_CLASSES, Config.TASK_B_CLASSES)\n",
        "\n",
        "        # Load weights with correct mapping\n",
        "        model_state = {}\n",
        "\n",
        "        # Handle different checkpoint formats\n",
        "        if 'model_state_dict' in checkpoint:\n",
        "            phase2_state = checkpoint['model_state_dict']\n",
        "        else:\n",
        "            phase2_state = checkpoint\n",
        "\n",
        "        for key, value in phase2_state.items():\n",
        "            if key.startswith('backbone.features'):\n",
        "                # backbone.features.X -> features.X\n",
        "                new_key = key.replace('backbone.', '')\n",
        "                model_state[new_key] = value\n",
        "            elif key.startswith('backbone.classifier'):\n",
        "                # backbone.classifier.X -> head_a.X\n",
        "                new_key = key.replace('backbone.classifier', 'head_a')\n",
        "                model_state[new_key] = value\n",
        "\n",
        "        # Load the mapped weights\n",
        "        model.load_state_dict(model_state, strict=False)\n",
        "        model = model.to(Config.device)\n",
        "        print(\"    Phase 2 model loaded\")\n",
        "        print(f\"   Verification: head_a.3.weight shape = {model.head_a[3].weight.shape}\")\n",
        "\n",
        "        return model\n",
        "    else:\n",
        "        raise FileNotFoundError(f\"Phase 2 model not found at {Config.PHASE2_MODEL_PATH}\")\n",
        "\n",
        "# ============================================================================\n",
        "# EVALUATION FUNCTIONS\n",
        "# ============================================================================\n",
        "def evaluate_task(model, dataloader, task, class_names):\n",
        "    \"\"\"Evaluate model on a specific task\"\"\"\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in dataloader:\n",
        "            images = images.to(Config.device)\n",
        "            outputs = model(images, task=task)\n",
        "            preds = outputs.argmax(dim=1)\n",
        "\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.numpy())\n",
        "\n",
        "    acc = accuracy_score(all_labels, all_preds)\n",
        "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
        "\n",
        "    return acc, f1, all_preds, all_labels\n",
        "\n",
        "def print_evaluation_report(acc, f1, preds, labels, class_names, task_name):\n",
        "    \"\"\"Print detailed evaluation report\"\"\"\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"üìä {task_name} EVALUATION\")\n",
        "    print(f\"{'='*70}\")\n",
        "    print(f\"   Accuracy:  {acc*100:.2f}%\")\n",
        "    print(f\"   F1-Score:  {f1:.4f}\")\n",
        "    print(f\"\\nüìã Classification Report:\")\n",
        "    print(classification_report(labels, preds, target_names=class_names, digits=4))\n",
        "\n",
        "# ============================================================================\n",
        "# TRAINING FUNCTION WITH LWF\n",
        "# ============================================================================\n",
        "def train_phase3_lwf():\n",
        "    \"\"\"Phase 3: Continual Learning with LwF\"\"\"\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"üöÄ PHASE 3: CONTINUAL LEARNING WITH LWF\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # Create save directory\n",
        "    Path(Config.SAVE_DIR).mkdir(exist_ok=True)\n",
        "\n",
        "    # Load Phase 2 model (student)\n",
        "    student_model = load_phase2_model()\n",
        "\n",
        "    # Create teacher model (frozen copy of Phase 2 model)\n",
        "    print(\"\\nüìö Creating teacher model (frozen copy)...\")\n",
        "    teacher_model = copy.deepcopy(student_model)\n",
        "    teacher_model.eval()  # Set to eval mode\n",
        "\n",
        "    # Freeze all teacher parameters\n",
        "    for param in teacher_model.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    print(\"   Teacher model created and frozen\")\n",
        "\n",
        "\n",
        "\n",
        "    # Create Task A test loader for retention evaluation\n",
        "    task_a_test_loader, task_a_classes = create_task_a_splits()\n",
        "\n",
        "    # Evaluate Task A before fine-tuning (baseline)\n",
        "    print(\"\\nüß™ Evaluating Task A (OCT) BEFORE adapting for task B\")\n",
        "    task_a_acc_before, task_a_f1_before, _, _ = evaluate_task(\n",
        "        student_model, task_a_test_loader, task='a', class_names=task_a_classes\n",
        "    )\n",
        "    print(f\"   Task A Accuracy: {task_a_acc_before*100:.2f}%\")\n",
        "    print(f\"   Task A F1: {task_a_f1_before:.4f}\")\n",
        "\n",
        "    # Create Task B dataloaders\n",
        "    train_loader, val_loader, test_loader, class_weights, task_b_classes = create_task_b_splits()\n",
        "\n",
        "    # Setup training\n",
        "    optimizer = optim.Adam([\n",
        "        {'params': student_model.features.parameters(), 'lr': 1e-5},\n",
        "        {'params': student_model.head_b.parameters(), 'lr': 1e-4}\n",
        "    ])\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5,\n",
        "                                                     patience=3)\n",
        "\n",
        "    # Training loop\n",
        "    best_val_f1 = 0.0\n",
        "    patience_counter = 0\n",
        "    history = {'train_loss': [], 'val_f1': [], 'task_a_f1': []}\n",
        "\n",
        "    print(f\"\\nüéØ Training Task B (Chest X-ray) with LwF (Œ±={Config.LWF_ALPHA}, T={Config.LWF_TEMPERATURE})...\")\n",
        "\n",
        "    for epoch in range(Config.NUM_EPOCHS):\n",
        "        # Training\n",
        "        student_model.train()\n",
        "        if Config.FREEZE_BATCHNORM:\n",
        "          student_model.features.eval()\n",
        "        student_model.head_a.eval()  # Keep Task A head frozen\n",
        "        train_loss = 0.0\n",
        "\n",
        "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{Config.NUM_EPOCHS}\")\n",
        "        for images, labels in pbar:\n",
        "            images, labels = images.to(Config.device), labels.to(Config.device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Student predictions on Task B (new task)\n",
        "            student_logits_b = student_model(images, task='b')\n",
        "\n",
        "            # Task B classification loss\n",
        "            ce_loss = criterion(student_logits_b, labels)\n",
        "\n",
        "            # Get teacher's predictions on Task A (to preserve old knowledge)\n",
        "            with torch.no_grad():\n",
        "                teacher_logits_a = teacher_model(images, task='a')\n",
        "\n",
        "            # Student's predictions on Task A (should match teacher)\n",
        "            student_logits_a = student_model(images, task='a')\n",
        "\n",
        "            # Distillation loss (preserve Task A knowledge)\n",
        "            distill_loss = distillation_loss(\n",
        "                student_logits_a,\n",
        "                teacher_logits_a,\n",
        "                Config.LWF_TEMPERATURE\n",
        "            )\n",
        "\n",
        "            # Total loss\n",
        "            total_loss = ce_loss + Config.LWF_ALPHA * distill_loss\n",
        "\n",
        "            total_loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += total_loss.item()\n",
        "            pbar.set_postfix({\n",
        "                'loss': f'{total_loss.item():.4f}',\n",
        "                'ce': f'{ce_loss.item():.4f}',\n",
        "                'distill': f'{distill_loss.item():.4f}'\n",
        "            })\n",
        "\n",
        "        avg_train_loss = train_loss / len(train_loader)\n",
        "        history['train_loss'].append(avg_train_loss)\n",
        "\n",
        "        # Validation on Task B\n",
        "        val_acc, val_f1, _, _ = evaluate_task(student_model, val_loader, task='b',\n",
        "                                             class_names=task_b_classes)\n",
        "        history['val_f1'].append(val_f1)\n",
        "\n",
        "        print(f\"\\n   Epoch {epoch+1} - Task B Val F1: {val_f1:.4f} | Acc: {val_acc*100:.2f}%\")\n",
        "\n",
        "        # Evaluate Task A retention periodically\n",
        "        if (epoch + 1) % Config.EVAL_TASK_A_EVERY == 0:\n",
        "            task_a_acc, task_a_f1, _, _ = evaluate_task(student_model, task_a_test_loader,\n",
        "                                                        task='a', class_names=task_a_classes)\n",
        "            history['task_a_f1'].append(task_a_f1)\n",
        "            retention = (task_a_f1 / task_a_f1_before) * 100\n",
        "            print(f\"   üìà Task A Retention: F1={task_a_f1:.4f} ({retention:.2f}% of baseline)\")\n",
        "\n",
        "        # Learning rate scheduling\n",
        "        scheduler.step(val_f1)\n",
        "\n",
        "        # Early stopping and checkpointing\n",
        "        if val_f1 > best_val_f1:\n",
        "            best_val_f1 = val_f1\n",
        "            patience_counter = 0\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': student_model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'val_f1': val_f1,\n",
        "                'task_a_f1_before': task_a_f1_before\n",
        "            }, f\"{Config.SAVE_DIR}/phase3_lwf_best.pth\")\n",
        "            print(f\"   üíæ Best model saved (Val F1: {val_f1:.4f})\")\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= Config.PATIENCE:\n",
        "                print(f\"\\n‚è∏Ô∏è  Early stopping triggered (patience={Config.PATIENCE})\")\n",
        "                break\n",
        "\n",
        "    # Final evaluation\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"üìä FINAL EVALUATION\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # Load best model\n",
        "    checkpoint = torch.load(f\"{Config.SAVE_DIR}/phase3_lwf_best.pth\")\n",
        "    student_model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "    # Task B (Chest X-ray) - Test set\n",
        "    task_b_acc, task_b_f1, task_b_preds, task_b_labels = evaluate_task(\n",
        "        student_model, test_loader, task='b', class_names=task_b_classes\n",
        "    )\n",
        "    print_evaluation_report(task_b_acc, task_b_f1, task_b_preds, task_b_labels,\n",
        "                          task_b_classes, \"TASK B (Chest X-ray)\")\n",
        "\n",
        "    # Task A (OCT) - Retention test\n",
        "    task_a_acc_after, task_a_f1_after, task_a_preds, task_a_labels = evaluate_task(\n",
        "        student_model, task_a_test_loader, task='a', class_names=task_a_classes\n",
        "    )\n",
        "    print_evaluation_report(task_a_acc_after, task_a_f1_after, task_a_preds, task_a_labels,\n",
        "                          task_a_classes, \"TASK A (OCT) - Retention Check\")\n",
        "\n",
        "    # Retention metrics\n",
        "    retention_f1 = (task_a_f1_after / task_a_f1_before) * 100\n",
        "    retention_acc = (task_a_acc_after / task_a_acc_before) * 100\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"üéØ CONTINUAL LEARNING SUMMARY\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"üìä Task A (OCT) Retention:\")\n",
        "    print(f\"   Before: F1={task_a_f1_before:.4f}, Acc={task_a_acc_before*100:.2f}%\")\n",
        "    print(f\"   After:  F1={task_a_f1_after:.4f}, Acc={task_a_acc_after*100:.2f}%\")\n",
        "    print(f\"   Retention: F1={retention_f1:.2f}%, Acc={retention_acc:.2f}%\")\n",
        "    print(f\"\\nüìä Task B (Chest X-ray) Performance:\")\n",
        "    print(f\"   Test F1: {task_b_f1:.4f}\")\n",
        "    print(f\"   Test Acc: {task_b_acc*100:.2f}%\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # Save confusion matrices\n",
        "    save_confusion_matrix(task_a_labels, task_a_preds, task_a_classes,\n",
        "                         \"Task A (OCT) - After LwF\", f\"{Config.SAVE_DIR}/cm_task_a.png\")\n",
        "    save_confusion_matrix(task_b_labels, task_b_preds, task_b_classes,\n",
        "                         \"Task B (Chest X-ray)\", f\"{Config.SAVE_DIR}/cm_task_b.png\")\n",
        "\n",
        "    return student_model, history\n",
        "\n",
        "def save_confusion_matrix(labels, preds, class_names, title, save_path):\n",
        "    \"\"\"Save confusion matrix plot\"\"\"\n",
        "    cm = confusion_matrix(labels, preds)\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=class_names, yticklabels=class_names)\n",
        "    plt.title(title)\n",
        "    plt.ylabel('True Label')\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "    print(f\"   ‚úÖ Confusion matrix saved: {save_path}\")\n",
        "\n",
        "# ============================================================================\n",
        "# MAIN EXECUTION\n",
        "# ============================================================================\n",
        "if __name__ == \"__main__\":\n",
        "    model, history = train_phase3_lwf()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lw3bQgO9T7I8"
      },
      "source": [
        "Freezing BatchNorm statistics during Task B training achieved 98.77% retention (97.29%‚Üí96.06%), nearly eliminating catastrophic forgetting while maintaining 97.07% Task B accuracy. All Task A classes retained balanced performance (93-98% recall), contrasting sharply with the severe class collapse observed without BN freezing. The minimal trade-off (Task B performance decreased by 2.55 percentage points compared to non-frozen BN) demonstrates that LwF's knowledge distillation is highly effective when normalization drift is controlled. Comparing with EWC results, both methods achieve similar retention (~99%) with BN freezing, indicating that BatchNorm drift accounts for ~37 percentage points of forgetting regardless of the continual learning algorithm. This validates that modern architectures require dual protection: algorithmic mechanisms (EWC/LwF) for weight/prediction-level forgetting and architectural modifications (BN freezing) for distribution-level forgetting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OvMCbdrwT9PC"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
