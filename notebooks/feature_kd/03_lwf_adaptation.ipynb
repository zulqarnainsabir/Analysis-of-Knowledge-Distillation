{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N_LUciuh0ttE",
        "outputId": "eeb5b55a-090f-4c8d-f938-c2dbe7b7a06a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1cIGCfx6CiVgEpq8PyKzmF1LBJiQGkxzc\n",
            "From (redirected): https://drive.google.com/uc?id=1cIGCfx6CiVgEpq8PyKzmF1LBJiQGkxzc&confirm=t&uuid=05b8ef28-56f5-4fef-84ad-1d7ceae88d0f\n",
            "To: /content/OCT2017.tar.gz\n",
            "100% 5.79G/5.79G [01:16<00:00, 75.6MB/s]\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1JobiELb-4mO_Gk3NY6eyIz-3oRw3U2zT\n",
            "From (redirected): https://drive.google.com/uc?id=1JobiELb-4mO_Gk3NY6eyIz-3oRw3U2zT&confirm=t&uuid=fd8ebfb2-2d7f-4f80-a55a-fc4994564589\n",
            "To: /content/ChestXRay2017.zip\n",
            "100% 1.24G/1.24G [00:19<00:00, 62.3MB/s]\n"
          ]
        }
      ],
      "source": [
        "!gdown --fuzzy \"https://drive.google.com/file/d/1cIGCfx6CiVgEpq8PyKzmF1LBJiQGkxzc/view?usp=sharing\"\n",
        "!gdown --fuzzy \"https://drive.google.com/file/d/1JobiELb-4mO_Gk3NY6eyIz-3oRw3U2zT/view?usp=sharing\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "8mf9FvWH1F92"
      },
      "outputs": [],
      "source": [
        "#Extract zip\n",
        "!tar -xzf \"/content/OCT2017.tar.gz\" -C /content/data/\n",
        "!unzip -q /content/ChestXRay2017.zip -d /content/data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tUlY6zQ_1LLO",
        "outputId": "7c13ff11-9332-46b3-9269-1d673dd6a779"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1F_vX0fmLL0nKlhaQMhJWHGcFu9a3NxEs\n",
            "From (redirected): https://drive.google.com/uc?id=1F_vX0fmLL0nKlhaQMhJWHGcFu9a3NxEs&confirm=t&uuid=f72c5ec5-b428-4ff1-b373-ca9cb3eb18e2\n",
            "To: /content/best_mobilenetv3_student_kd.pth\n",
            "100% 39.0M/39.0M [00:00<00:00, 120MB/s] \n"
          ]
        }
      ],
      "source": [
        "!gdown --fuzzy \"https://drive.google.com/file/d/1F_vX0fmLL0nKlhaQMhJWHGcFu9a3NxEs/view?usp=sharing\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JcIOJzD01PST",
        "outputId": "1356ce89-8fc7-4dfc-d6f9-f2972093defc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "üöÄ PHASE 3: CONTINUAL LEARNING WITH LWF\n",
            "   Model: MobileNetV3 (Feature-Based KD from Phase 2)\n",
            "======================================================================\n",
            "\n",
            "üìÇ Loading Phase 2 Feature-Based KD model...\n",
            "   Phase 2 model loaded\n",
            "  Phase 2 Test F1: N/A\n",
            "  Phase 2 Test Acc: N/A\n",
            "  Verification: head_a.3.weight shape = torch.Size([4, 256])\n",
            "  New Task B head initialized: head_b.3.weight shape = torch.Size([2, 256])\n",
            "\n",
            "üìö Creating teacher model (frozen copy)...\n",
            "   ‚úÖ Teacher model created and frozen\n",
            "\n",
            "üìä Creating Task A (OCT) evaluation splits...\n",
            "   Total Task A samples: 83,484\n",
            "   Classes: ['CNV', 'DME', 'DRUSEN', 'NORMAL']\n",
            "   Class distribution: {0: 37205, 1: 11348, 2: 8616, 3: 26315}\n",
            "   Train: 58,438 | Val: 12,523 | Test: 12,523\n",
            "\n",
            "üß™ Evaluating Task A (OCT) BEFORE adapting to Task B...\n",
            "   Task A Accuracy: 97.06%\n",
            "   Task A F1: 0.9706\n",
            "\n",
            "üìÇ Creating Task B (Chest X-ray) splits...\n",
            "   Total samples: 5,232\n",
            "   Classes: ['NORMAL', 'PNEUMONIA']\n",
            "   Class distribution: {0: 1349, 1: 3883}\n",
            "   Train: 3,662 | Val: 785 | Test: 785\n",
            "   Class weights: [1.9396186 0.6736571]\n",
            "\n",
            "üéØ Training Task B (Chest X-ray) with LwF...\n",
            "   Alpha (Œ±): 2.0\n",
            "   Temperature (T): 2.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 29/29 [01:12<00:00,  2.50s/it, loss=4.8426, ce=0.4788, distill=2.1819]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "   Epoch 1 - Task B Val F1: 0.4665 | Acc: 47.52%\n",
            "   üíæ Best model saved (Val F1: 0.4665)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 29/29 [01:08<00:00,  2.37s/it, loss=3.1166, ce=0.4570, distill=1.3298]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "   Epoch 2 - Task B Val F1: 0.6729 | Acc: 65.48%\n",
            "   üìà Task A Retention: F1=0.8181 (84.28% of baseline)\n",
            "   üíæ Best model saved (Val F1: 0.6729)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 29/29 [01:08<00:00,  2.37s/it, loss=3.4063, ce=0.4165, distill=1.4949]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "   Epoch 3 - Task B Val F1: 0.8121 | Acc: 80.00%\n",
            "   üíæ Best model saved (Val F1: 0.8121)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 29/29 [01:08<00:00,  2.37s/it, loss=3.1142, ce=0.3867, distill=1.3637]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "   Epoch 4 - Task B Val F1: 0.8518 | Acc: 84.33%\n",
            "   üìà Task A Retention: F1=0.6550 (67.48% of baseline)\n",
            "   üíæ Best model saved (Val F1: 0.8518)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 29/29 [01:09<00:00,  2.38s/it, loss=2.4044, ce=0.3242, distill=1.0401]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "   Epoch 5 - Task B Val F1: 0.8520 | Acc: 84.33%\n",
            "   üíæ Best model saved (Val F1: 0.8520)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 6/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 29/29 [01:07<00:00,  2.34s/it, loss=2.3552, ce=0.2649, distill=1.0451]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "   Epoch 6 - Task B Val F1: 0.8660 | Acc: 85.86%\n",
            "   üìà Task A Retention: F1=0.5788 (59.63% of baseline)\n",
            "   üíæ Best model saved (Val F1: 0.8660)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 7/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 29/29 [01:07<00:00,  2.34s/it, loss=1.7539, ce=0.3324, distill=0.7108]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "   Epoch 7 - Task B Val F1: 0.8824 | Acc: 87.64%\n",
            "   üíæ Best model saved (Val F1: 0.8824)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 8/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 29/29 [01:08<00:00,  2.36s/it, loss=1.5656, ce=0.2191, distill=0.6732]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "   Epoch 8 - Task B Val F1: 0.9070 | Acc: 90.32%\n",
            "   üìà Task A Retention: F1=0.5577 (57.46% of baseline)\n",
            "   üíæ Best model saved (Val F1: 0.9070)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 9/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 29/29 [01:08<00:00,  2.37s/it, loss=1.5973, ce=0.2829, distill=0.6572]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "   Epoch 9 - Task B Val F1: 0.8883 | Acc: 88.28%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 10/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 29/29 [01:08<00:00,  2.37s/it, loss=1.4294, ce=0.2555, distill=0.5869]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "   Epoch 10 - Task B Val F1: 0.9011 | Acc: 89.68%\n",
            "   üìà Task A Retention: F1=0.5603 (57.73% of baseline)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 11/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 29/29 [01:09<00:00,  2.40s/it, loss=1.3337, ce=0.2240, distill=0.5549]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "   Epoch 11 - Task B Val F1: 0.9128 | Acc: 90.96%\n",
            "   üíæ Best model saved (Val F1: 0.9128)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 12/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 29/29 [01:08<00:00,  2.37s/it, loss=1.1249, ce=0.1827, distill=0.4711]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "   Epoch 12 - Task B Val F1: 0.9117 | Acc: 90.83%\n",
            "   üìà Task A Retention: F1=0.5687 (58.59% of baseline)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 13/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 29/29 [01:09<00:00,  2.38s/it, loss=1.1468, ce=0.3129, distill=0.4170]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "   Epoch 13 - Task B Val F1: 0.9141 | Acc: 91.08%\n",
            "   üíæ Best model saved (Val F1: 0.9141)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 14/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 29/29 [01:09<00:00,  2.40s/it, loss=0.9299, ce=0.0598, distill=0.4351]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "   Epoch 14 - Task B Val F1: 0.9117 | Acc: 90.83%\n",
            "   üìà Task A Retention: F1=0.5752 (59.26% of baseline)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 15/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 29/29 [01:08<00:00,  2.37s/it, loss=0.6769, ce=0.1569, distill=0.2600]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "   Epoch 15 - Task B Val F1: 0.9106 | Acc: 90.70%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 16/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 29/29 [01:09<00:00,  2.40s/it, loss=0.7077, ce=0.2324, distill=0.2376]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "   Epoch 16 - Task B Val F1: 0.9211 | Acc: 91.85%\n",
            "   üìà Task A Retention: F1=0.5851 (60.29% of baseline)\n",
            "   üíæ Best model saved (Val F1: 0.9211)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 17/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 29/29 [01:08<00:00,  2.36s/it, loss=0.7431, ce=0.1455, distill=0.2988]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "   Epoch 17 - Task B Val F1: 0.9223 | Acc: 91.97%\n",
            "   üíæ Best model saved (Val F1: 0.9223)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 18/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 29/29 [01:08<00:00,  2.36s/it, loss=0.7627, ce=0.1968, distill=0.2830]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "   Epoch 18 - Task B Val F1: 0.9165 | Acc: 91.34%\n",
            "   üìà Task A Retention: F1=0.5951 (61.31% of baseline)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 19/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 29/29 [01:08<00:00,  2.37s/it, loss=0.5391, ce=0.1027, distill=0.2182]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "   Epoch 19 - Task B Val F1: 0.9212 | Acc: 91.85%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 20/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 29/29 [01:08<00:00,  2.37s/it, loss=0.6072, ce=0.0684, distill=0.2694]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "   Epoch 20 - Task B Val F1: 0.9166 | Acc: 91.34%\n",
            "   üìà Task A Retention: F1=0.6048 (62.31% of baseline)\n",
            "\n",
            "======================================================================\n",
            "üìä FINAL EVALUATION\n",
            "======================================================================\n",
            "\n",
            "======================================================================\n",
            "üìä TASK B (Chest X-ray) EVALUATION\n",
            "======================================================================\n",
            "   Accuracy:  94.27%\n",
            "   F1-Score:  0.9439\n",
            "\n",
            "üìã Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      NORMAL     0.8405    0.9606    0.8966       203\n",
            "   PNEUMONIA     0.9855    0.9364    0.9604       582\n",
            "\n",
            "    accuracy                         0.9427       785\n",
            "   macro avg     0.9130    0.9485    0.9285       785\n",
            "weighted avg     0.9480    0.9427    0.9439       785\n",
            "\n",
            "\n",
            "======================================================================\n",
            "üìä TASK A (OCT) - Retention Check EVALUATION\n",
            "======================================================================\n",
            "   Accuracy:  60.58%\n",
            "   F1-Score:  0.5902\n",
            "\n",
            "üìã Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         CNV     0.9329    0.3962    0.5562      5581\n",
            "         DME     0.9554    0.2515    0.3981      1702\n",
            "      DRUSEN     0.2911    0.7989    0.4267      1293\n",
            "      NORMAL     0.6358    0.9916    0.7748      3947\n",
            "\n",
            "    accuracy                         0.6058     12523\n",
            "   macro avg     0.7038    0.6095    0.5389     12523\n",
            "weighted avg     0.7760    0.6058    0.5902     12523\n",
            "\n",
            "\n",
            "======================================================================\n",
            "üéØ CONTINUAL LEARNING SUMMARY\n",
            "======================================================================\n",
            "üìä Task A (OCT) Retention:\n",
            "   Before: F1=0.9706, Acc=97.06%\n",
            "   After:  F1=0.5902, Acc=60.58%\n",
            "   Retention: F1=60.81%, Acc=62.41%\n",
            "\n",
            "üìä Task B (Chest X-ray) Performance:\n",
            "   Test F1: 0.9439\n",
            "   Test Acc: 94.27%\n",
            "======================================================================\n",
            "   ‚úÖ Confusion matrix saved: /content/phase3_lwf_feature_kd_results/cm_task_a.png\n",
            "   ‚úÖ Confusion matrix saved: /content/phase3_lwf_feature_kd_results/cm_task_b.png\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "import torchvision.models as models\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, f1_score, accuracy_score, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from collections import Counter\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "import copy\n",
        "from PIL import Image\n",
        "\n",
        "# ============================================================================\n",
        "# CONFIGURATION\n",
        "# ============================================================================\n",
        "class Config:\n",
        "    # Paths\n",
        "    TASK_A_DATA_PATH = \"/content/data/OCT2017/train\"  # OCT images folder\n",
        "    TASK_B_DATA_PATH = \"/content/data/chest_xray/train\"  # Chest X-ray images folder\n",
        "    PHASE2_MODEL_PATH = \"/content/best_mobilenetv3_student_kd.pth\"\n",
        "    SAVE_DIR = \"/content/phase3_lwf_feature_kd_results\"\n",
        "\n",
        "    # Model settings\n",
        "    TASK_A_CLASSES = 4  # OCT classes\n",
        "    TASK_B_CLASSES = 2  # Chest X-ray classes\n",
        "\n",
        "    # LwF hyperparameters\n",
        "    LWF_ALPHA = 2.0  # Distillation loss weight (tune: 1.0-10.0)\n",
        "    LWF_TEMPERATURE = 2.0\n",
        "\n",
        "    # Training hyperparameters\n",
        "    BATCH_SIZE = 128\n",
        "    NUM_EPOCHS = 20\n",
        "    PATIENCE = 5  # Early stopping\n",
        "\n",
        "    # Data augmentation\n",
        "    USE_AUGMENTATION = True\n",
        "\n",
        "    # Evaluation\n",
        "    EVAL_TASK_A_EVERY = 2  # Evaluate Task A retention every N epochs\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# ============================================================================\n",
        "# DATA LOADING UTILITIES\n",
        "# ============================================================================\n",
        "def load_task_paths(data_path):\n",
        "    \"\"\"\n",
        "    Universal data loader for both OCT and Chest X-ray\n",
        "    Loads from: /path/to/train/CLASS_NAME/*.jpg\n",
        "    \"\"\"\n",
        "    data_path = Path(data_path)\n",
        "\n",
        "    # Get all class folders\n",
        "    class_names = sorted([d.name for d in data_path.iterdir() if d.is_dir()])\n",
        "\n",
        "    all_paths = []\n",
        "    all_labels = []\n",
        "\n",
        "    for idx, class_name in enumerate(class_names):\n",
        "        class_dir = data_path / class_name\n",
        "        # Support multiple image formats\n",
        "        paths = list(class_dir.glob('*.jpeg')) + \\\n",
        "                list(class_dir.glob('*.jpg')) + \\\n",
        "                list(class_dir.glob('*.png'))\n",
        "\n",
        "        all_paths.extend(paths)\n",
        "        all_labels.extend([idx] * len(paths))\n",
        "\n",
        "    return all_paths, all_labels, class_names\n",
        "\n",
        "# ============================================================================\n",
        "# DATASET CLASS\n",
        "# ============================================================================\n",
        "class ImageDataset(Dataset):\n",
        "    def __init__(self, paths, labels, transform=None):\n",
        "        self.paths = paths\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img = Image.open(self.paths[idx]).convert('RGB')\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        return img, label\n",
        "\n",
        "# ============================================================================\n",
        "# DATA SPLITS\n",
        "# ============================================================================\n",
        "def create_task_a_splits():\n",
        "    \"\"\"Create stratified splits for Task A (OCT)\"\"\"\n",
        "    print(\"\\nüìä Creating Task A (OCT) evaluation splits...\")\n",
        "\n",
        "    all_paths, all_labels, task_a_class_names = load_task_paths(Config.TASK_A_DATA_PATH)\n",
        "    print(f\"   Total Task A samples: {len(all_paths):,}\")\n",
        "    print(f\"   Classes: {task_a_class_names}\")\n",
        "\n",
        "    # Class distribution\n",
        "    class_counts = Counter(all_labels)\n",
        "    print(f\"   Class distribution: {dict(class_counts)}\")\n",
        "\n",
        "    # 70/15/15 split\n",
        "    train_paths, temp_paths, train_labels, temp_labels = train_test_split(\n",
        "        all_paths, all_labels, test_size=0.30, stratify=all_labels, random_state=42\n",
        "    )\n",
        "\n",
        "    val_paths, test_paths, val_labels, test_labels = train_test_split(\n",
        "        temp_paths, temp_labels, test_size=0.50, stratify=temp_labels, random_state=42\n",
        "    )\n",
        "\n",
        "    print(f\"   Train: {len(train_paths):,} | Val: {len(val_paths):,} | Test: {len(test_paths):,}\")\n",
        "\n",
        "    test_transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    test_dataset = ImageDataset(test_paths, test_labels, test_transform)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=Config.BATCH_SIZE,\n",
        "                            shuffle=False, num_workers=2)\n",
        "\n",
        "    return test_loader, task_a_class_names\n",
        "\n",
        "def create_task_b_splits():\n",
        "    \"\"\"Create stratified splits for Task B (Chest X-ray)\"\"\"\n",
        "    print(\"\\nüìÇ Creating Task B (Chest X-ray) splits...\")\n",
        "\n",
        "    all_paths, all_labels, task_b_class_names = load_task_paths(Config.TASK_B_DATA_PATH)\n",
        "    print(f\"   Total samples: {len(all_paths):,}\")\n",
        "    print(f\"   Classes: {task_b_class_names}\")\n",
        "\n",
        "    # Class distribution\n",
        "    class_counts = Counter(all_labels)\n",
        "    print(f\"   Class distribution: {dict(class_counts)}\")\n",
        "\n",
        "    # 70/15/15 stratified split\n",
        "    train_paths, temp_paths, train_labels, temp_labels = train_test_split(\n",
        "        all_paths, all_labels, test_size=0.30, stratify=all_labels, random_state=42\n",
        "    )\n",
        "\n",
        "    val_paths, test_paths, val_labels, test_labels = train_test_split(\n",
        "        temp_paths, temp_labels, test_size=0.50, stratify=temp_labels, random_state=42\n",
        "    )\n",
        "\n",
        "    print(f\"   Train: {len(train_paths):,} | Val: {len(val_paths):,} | Test: {len(test_paths):,}\")\n",
        "\n",
        "    # Compute class weights for imbalanced dataset\n",
        "    train_class_counts = Counter(train_labels)\n",
        "    total_samples = len(train_labels)\n",
        "    class_weights = torch.tensor([\n",
        "        total_samples / (len(train_class_counts) * train_class_counts[i])\n",
        "        for i in range(len(task_b_class_names))\n",
        "    ], dtype=torch.float32).to(Config.device)\n",
        "\n",
        "    print(f\"   Class weights: {class_weights.cpu().numpy()}\")\n",
        "\n",
        "    # Data transforms with augmentation\n",
        "    if Config.USE_AUGMENTATION:\n",
        "        train_transform = transforms.Compose([\n",
        "            transforms.Resize((224, 224)),\n",
        "            transforms.RandomHorizontalFlip(p=0.5),\n",
        "            transforms.RandomRotation(10),\n",
        "            transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "        ])\n",
        "    else:\n",
        "        train_transform = transforms.Compose([\n",
        "            transforms.Resize((224, 224)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "        ])\n",
        "\n",
        "    val_transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    train_dataset = ImageDataset(train_paths, train_labels, train_transform)\n",
        "    val_dataset = ImageDataset(val_paths, val_labels, val_transform)\n",
        "    test_dataset = ImageDataset(test_paths, test_labels, val_transform)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=Config.BATCH_SIZE,\n",
        "                             shuffle=True, num_workers=2)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=Config.BATCH_SIZE,\n",
        "                           shuffle=False, num_workers=2)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=Config.BATCH_SIZE,\n",
        "                            shuffle=False, num_workers=2)\n",
        "\n",
        "    return train_loader, val_loader, test_loader, class_weights, task_b_class_names\n",
        "\n",
        "# ============================================================================\n",
        "# MULTI-HEAD MODEL (Based on Phase 2 Feature-Based KD Architecture)\n",
        "# ============================================================================\n",
        "class MultiHeadMobileNetV3WithFeatures(nn.Module):\n",
        "    \"\"\"\n",
        "    Multi-head MobileNetV3 for continual learning\n",
        "    Based on Phase 2's feature-based KD architecture\n",
        "    \"\"\"\n",
        "    def __init__(self, num_classes_a=4, num_classes_b=2):\n",
        "        super().__init__()\n",
        "\n",
        "        # Load MobileNetV3-Large backbone (same as Phase 2)\n",
        "        mobilenet = models.mobilenet_v3_large(weights=None)\n",
        "        self.features = mobilenet.features  # Shared backbone\n",
        "        self.avgpool = mobilenet.avgpool\n",
        "\n",
        "        # Feature dimension from MobileNetV3-Large\n",
        "        self.feature_dim = 960\n",
        "\n",
        "        # Task A head (OCT) - EXACT same structure as Phase 2\n",
        "        self.head_a = nn.Sequential(\n",
        "            nn.Linear(self.feature_dim, 256),\n",
        "            nn.Hardswish(inplace=True),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(256, num_classes_a)\n",
        "        )\n",
        "\n",
        "        # Task B head (Chest X-ray) - new head for new task\n",
        "        self.head_b = nn.Sequential(\n",
        "            nn.Linear(self.feature_dim, 256),\n",
        "            nn.Hardswish(inplace=True),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(256, num_classes_b)\n",
        "        )\n",
        "\n",
        "        # Feature projector (same as Phase 2,)\n",
        "        self.feature_projector = nn.Sequential(\n",
        "            nn.Linear(self.feature_dim, 1024),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(1024, 2048)\n",
        "        )\n",
        "\n",
        "    def extract_features(self, x):\n",
        "        \"\"\"Extract features before the final classifier\"\"\"\n",
        "        x = self.features(x)\n",
        "        x = self.avgpool(x)\n",
        "        features = torch.flatten(x, 1)  # [batch_size, 960]\n",
        "        return features\n",
        "\n",
        "    def forward(self, x, task='b', return_features=False):\n",
        "        \"\"\"\n",
        "        Forward pass with task selection\n",
        "\n",
        "        Args:\n",
        "            x: input tensor\n",
        "            task: 'a' for OCT, 'b' for Chest X-ray\n",
        "            return_features: if True, return (logits, features)\n",
        "        \"\"\"\n",
        "        features = self.extract_features(x)\n",
        "\n",
        "        if task == 'a':\n",
        "            logits = self.head_a(features)\n",
        "        elif task == 'b':\n",
        "            logits = self.head_b(features)\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown task: {task}\")\n",
        "\n",
        "        if return_features:\n",
        "            return logits, features\n",
        "        return logits\n",
        "\n",
        "# ============================================================================\n",
        "# LWF DISTILLATION LOSS\n",
        "# ============================================================================\n",
        "def distillation_loss(student_logits, teacher_logits, temperature):\n",
        "    \"\"\"\n",
        "    Compute knowledge distillation loss\n",
        "\n",
        "    Args:\n",
        "        student_logits: Raw logits from student model\n",
        "        teacher_logits: Raw logits from teacher model (frozen)\n",
        "        temperature: Temperature for softening probabilities\n",
        "\n",
        "    Returns:\n",
        "        Distillation loss (KL divergence between soft targets)\n",
        "    \"\"\"\n",
        "    # Soften probabilities with temperature\n",
        "    student_soft = nn.functional.log_softmax(student_logits / temperature, dim=1)\n",
        "    teacher_soft = nn.functional.softmax(teacher_logits / temperature, dim=1)\n",
        "\n",
        "    # KL divergence loss\n",
        "    kl_div = nn.functional.kl_div(\n",
        "        student_soft,\n",
        "        teacher_soft,\n",
        "        reduction='batchmean'\n",
        "    )\n",
        "\n",
        "    # Scale by temperature^2\n",
        "    return kl_div * (temperature ** 2)\n",
        "\n",
        "# ============================================================================\n",
        "# LOAD PHASE 2 MODEL\n",
        "# ============================================================================\n",
        "def load_phase2_model():\n",
        "    \"\"\"Load Phase 2 Feature-Based KD model\"\"\"\n",
        "    print(\"\\nüìÇ Loading Phase 2 Feature-Based KD model...\")\n",
        "\n",
        "    if not Path(Config.PHASE2_MODEL_PATH).exists():\n",
        "        raise FileNotFoundError(f\"Phase 2 model not found at {Config.PHASE2_MODEL_PATH}\")\n",
        "\n",
        "    checkpoint = torch.load(Config.PHASE2_MODEL_PATH, map_location=Config.device)\n",
        "\n",
        "    # Create multi-head model\n",
        "    model = MultiHeadMobileNetV3WithFeatures(\n",
        "        num_classes_a=Config.TASK_A_CLASSES,\n",
        "        num_classes_b=Config.TASK_B_CLASSES\n",
        "    )\n",
        "\n",
        "    # Load weights with correct mapping from Phase 2\n",
        "    model_state = {}\n",
        "\n",
        "    if 'model_state_dict' in checkpoint:\n",
        "        phase2_state = checkpoint['model_state_dict']\n",
        "    else:\n",
        "        phase2_state = checkpoint\n",
        "\n",
        "    for key, value in phase2_state.items():\n",
        "        if key.startswith('backbone.features'):\n",
        "            # backbone.features.X -> features.X\n",
        "            new_key = key.replace('backbone.', '')\n",
        "            model_state[new_key] = value\n",
        "        elif key.startswith('backbone.classifier'):\n",
        "            # backbone.classifier.X -> head_a.X\n",
        "            new_key = key.replace('backbone.classifier', 'head_a')\n",
        "            model_state[new_key] = value\n",
        "        elif key == 'feature_projector.0.weight' or key == 'feature_projector.0.bias' or \\\n",
        "             key == 'feature_projector.2.weight' or key == 'feature_projector.2.bias':\n",
        "            # Keep feature projector weights (compatibility)\n",
        "            model_state[key] = value\n",
        "\n",
        "    # Load the mapped weights (strict=False because head_b is new)\n",
        "    missing_keys, unexpected_keys = model.load_state_dict(model_state, strict=False)\n",
        "    model = model.to(Config.device)\n",
        "\n",
        "    print(\"   Phase 2 model loaded\")\n",
        "    print(f\"  Phase 2 Test F1: {checkpoint.get('test_f1', 'N/A')}\")\n",
        "    print(f\"  Phase 2 Test Acc: {checkpoint.get('test_acc', 'N/A')}\")\n",
        "    print(f\"  Verification: head_a.3.weight shape = {model.head_a[3].weight.shape}\")\n",
        "    print(f\"  New Task B head initialized: head_b.3.weight shape = {model.head_b[3].weight.shape}\")\n",
        "\n",
        "    return model\n",
        "\n",
        "# ============================================================================\n",
        "# EVALUATION FUNCTIONS\n",
        "# ============================================================================\n",
        "def evaluate_task(model, dataloader, task, class_names):\n",
        "    \"\"\"Evaluate model on a specific task\"\"\"\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in dataloader:\n",
        "            images = images.to(Config.device)\n",
        "            outputs = model(images, task=task)\n",
        "            preds = outputs.argmax(dim=1)\n",
        "\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.numpy())\n",
        "\n",
        "    acc = accuracy_score(all_labels, all_preds)\n",
        "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
        "\n",
        "    return acc, f1, all_preds, all_labels\n",
        "\n",
        "def print_evaluation_report(acc, f1, preds, labels, class_names, task_name):\n",
        "    \"\"\"Print detailed evaluation report\"\"\"\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"üìä {task_name} EVALUATION\")\n",
        "    print(f\"{'='*70}\")\n",
        "    print(f\"   Accuracy:  {acc*100:.2f}%\")\n",
        "    print(f\"   F1-Score:  {f1:.4f}\")\n",
        "    print(f\"\\nüìã Classification Report:\")\n",
        "    print(classification_report(labels, preds, target_names=class_names, digits=4))\n",
        "\n",
        "# ============================================================================\n",
        "# TRAINING FUNCTION WITH LWF\n",
        "# ============================================================================\n",
        "def train_phase3_lwf():\n",
        "    \"\"\"Phase 3: Continual Learning with LwF for Feature-Based KD Student\"\"\"\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"üöÄ PHASE 3: CONTINUAL LEARNING WITH LWF\")\n",
        "    print(\"   Model: MobileNetV3 (Feature-Based KD from Phase 2)\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # Create save directory\n",
        "    Path(Config.SAVE_DIR).mkdir(exist_ok=True)\n",
        "\n",
        "    # Load Phase 2 model (student)\n",
        "    student_model = load_phase2_model()\n",
        "\n",
        "    # Create teacher model (frozen copy of Phase 2 model)\n",
        "    print(\"\\nüìö Creating teacher model (frozen copy)...\")\n",
        "    teacher_model = copy.deepcopy(student_model)\n",
        "    teacher_model.eval()  # Set to eval mode\n",
        "\n",
        "    # Freeze all teacher parameters\n",
        "    for param in teacher_model.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    print(\"   ‚úÖ Teacher model created and frozen\")\n",
        "\n",
        "    # Create Task A test loader for retention evaluation\n",
        "    task_a_test_loader, task_a_classes = create_task_a_splits()\n",
        "\n",
        "    # Evaluate Task A before fine-tuning (baseline)\n",
        "    print(\"\\nüß™ Evaluating Task A (OCT) BEFORE adapting to Task B...\")\n",
        "    task_a_acc_before, task_a_f1_before, _, _ = evaluate_task(\n",
        "        student_model, task_a_test_loader, task='a', class_names=task_a_classes\n",
        "    )\n",
        "    print(f\"   Task A Accuracy: {task_a_acc_before*100:.2f}%\")\n",
        "    print(f\"   Task A F1: {task_a_f1_before:.4f}\")\n",
        "\n",
        "    # Create Task B dataloaders\n",
        "    train_loader, val_loader, test_loader, class_weights, task_b_classes = create_task_b_splits()\n",
        "\n",
        "    # Setup training - ONLY train shared features and Task B head\n",
        "    # Task A head remains FROZEN to preserve its weights\n",
        "    optimizer = optim.Adam([\n",
        "        {'params': student_model.features.parameters(), 'lr': 1e-5},  # Fine-tune backbone\n",
        "        {'params': student_model.head_b.parameters(), 'lr': 1e-4}     # Train new head\n",
        "    ])\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5,\n",
        "                                                     patience=3)\n",
        "\n",
        "    # Training loop\n",
        "    best_val_f1 = 0.0\n",
        "    patience_counter = 0\n",
        "    history = {'train_loss': [], 'val_f1': [], 'task_a_f1': []}\n",
        "\n",
        "    print(f\"\\n Training Task B (Chest X-ray) with LwF...\")\n",
        "    print(f\"   Alpha (Œ±): {Config.LWF_ALPHA}\")\n",
        "    print(f\"   Temperature (T): {Config.LWF_TEMPERATURE}\")\n",
        "\n",
        "    for epoch in range(Config.NUM_EPOCHS):\n",
        "        # Training\n",
        "        student_model.train()\n",
        "        # Keep Task A head in eval mode (frozen)\n",
        "        student_model.head_a.eval()\n",
        "\n",
        "        train_loss = 0.0\n",
        "\n",
        "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{Config.NUM_EPOCHS}\")\n",
        "        for images, labels in pbar:\n",
        "            images, labels = images.to(Config.device), labels.to(Config.device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Student predictions on Task B (new task)\n",
        "            student_logits_b = student_model(images, task='b')\n",
        "\n",
        "            # Task B classification loss\n",
        "            ce_loss = criterion(student_logits_b, labels)\n",
        "\n",
        "            # Get teacher's predictions on Task A (to preserve old knowledge)\n",
        "            with torch.no_grad():\n",
        "                teacher_logits_a = teacher_model(images, task='a')\n",
        "\n",
        "            # Student's predictions on Task A (we want these to stay similar to teacher)\n",
        "            student_logits_a = student_model(images, task='a')\n",
        "\n",
        "            # Distillation loss (preserve Task A knowledge)\n",
        "            distill_loss = distillation_loss(\n",
        "                student_logits_a,\n",
        "                teacher_logits_a,\n",
        "                Config.LWF_TEMPERATURE\n",
        "            )\n",
        "\n",
        "            # Total loss = Task B loss + Œ± * Distillation loss\n",
        "            total_loss = ce_loss + Config.LWF_ALPHA * distill_loss\n",
        "\n",
        "            total_loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += total_loss.item()\n",
        "            pbar.set_postfix({\n",
        "                'loss': f'{total_loss.item():.4f}',\n",
        "                'ce': f'{ce_loss.item():.4f}',\n",
        "                'distill': f'{distill_loss.item():.4f}'\n",
        "            })\n",
        "\n",
        "        avg_train_loss = train_loss / len(train_loader)\n",
        "        history['train_loss'].append(avg_train_loss)\n",
        "\n",
        "        # Validation on Task B\n",
        "        val_acc, val_f1, _, _ = evaluate_task(student_model, val_loader, task='b',\n",
        "                                             class_names=task_b_classes)\n",
        "        history['val_f1'].append(val_f1)\n",
        "\n",
        "        print(f\"\\n   Epoch {epoch+1} - Task B Val F1: {val_f1:.4f} | Acc: {val_acc*100:.2f}%\")\n",
        "\n",
        "        # Evaluate Task A retention periodically\n",
        "        if (epoch + 1) % Config.EVAL_TASK_A_EVERY == 0:\n",
        "            task_a_acc, task_a_f1, _, _ = evaluate_task(student_model, task_a_test_loader,\n",
        "                                                        task='a', class_names=task_a_classes)\n",
        "            history['task_a_f1'].append(task_a_f1)\n",
        "            retention = (task_a_f1 / task_a_f1_before) * 100\n",
        "            print(f\"    Task A Retention: F1={task_a_f1:.4f} ({retention:.2f}% of baseline)\")\n",
        "\n",
        "        # Learning rate scheduling\n",
        "        scheduler.step(val_f1)\n",
        "\n",
        "        # Early stopping and checkpointing\n",
        "        if val_f1 > best_val_f1:\n",
        "            best_val_f1 = val_f1\n",
        "            patience_counter = 0\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': student_model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'val_f1': val_f1,\n",
        "                'task_a_f1_before': task_a_f1_before,\n",
        "                'lwf_alpha': Config.LWF_ALPHA,\n",
        "                'lwf_temperature': Config.LWF_TEMPERATURE\n",
        "            }, f\"{Config.SAVE_DIR}/phase3_lwf_best.pth\")\n",
        "            print(f\"   üíæ Best model saved (Val F1: {val_f1:.4f})\")\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= Config.PATIENCE:\n",
        "                print(f\"\\n‚è∏Ô∏è  Early stopping triggered (patience={Config.PATIENCE})\")\n",
        "                break\n",
        "\n",
        "    # Final evaluation\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"üìä FINAL EVALUATION\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # Load best model\n",
        "    checkpoint = torch.load(f\"{Config.SAVE_DIR}/phase3_lwf_best.pth\")\n",
        "    student_model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "    # Task B (Chest X-ray) - Test set\n",
        "    task_b_acc, task_b_f1, task_b_preds, task_b_labels = evaluate_task(\n",
        "        student_model, test_loader, task='b', class_names=task_b_classes\n",
        "    )\n",
        "    print_evaluation_report(task_b_acc, task_b_f1, task_b_preds, task_b_labels,\n",
        "                          task_b_classes, \"TASK B (Chest X-ray)\")\n",
        "\n",
        "    # Task A (OCT) - Retention test\n",
        "    task_a_acc_after, task_a_f1_after, task_a_preds, task_a_labels = evaluate_task(\n",
        "        student_model, task_a_test_loader, task='a', class_names=task_a_classes\n",
        "    )\n",
        "    print_evaluation_report(task_a_acc_after, task_a_f1_after, task_a_preds, task_a_labels,\n",
        "                          task_a_classes, \"TASK A (OCT) - Retention Check\")\n",
        "\n",
        "    # Retention metrics\n",
        "    retention_f1 = (task_a_f1_after / task_a_f1_before) * 100\n",
        "    retention_acc = (task_a_acc_after / task_a_acc_before) * 100\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"üéØ CONTINUAL LEARNING SUMMARY\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"üìä Task A (OCT) Retention:\")\n",
        "    print(f\"   Before: F1={task_a_f1_before:.4f}, Acc={task_a_acc_before*100:.2f}%\")\n",
        "    print(f\"   After:  F1={task_a_f1_after:.4f}, Acc={task_a_acc_after*100:.2f}%\")\n",
        "    print(f\"   Retention: F1={retention_f1:.2f}%, Acc={retention_acc:.2f}%\")\n",
        "    print(f\"\\nüìä Task B (Chest X-ray) Performance:\")\n",
        "    print(f\"   Test F1: {task_b_f1:.4f}\")\n",
        "    print(f\"   Test Acc: {task_b_acc*100:.2f}%\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # Save confusion matrices\n",
        "    save_confusion_matrix(task_a_labels, task_a_preds, task_a_classes,\n",
        "                         \"Task A (OCT) - After LwF\", f\"{Config.SAVE_DIR}/cm_task_a.png\")\n",
        "    save_confusion_matrix(task_b_labels, task_b_preds, task_b_classes,\n",
        "                         \"Task B (Chest X-ray)\", f\"{Config.SAVE_DIR}/cm_task_b.png\")\n",
        "\n",
        "    return student_model, history\n",
        "\n",
        "def save_confusion_matrix(labels, preds, class_names, title, save_path):\n",
        "    \"\"\"Save confusion matrix plot\"\"\"\n",
        "    cm = confusion_matrix(labels, preds)\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=class_names, yticklabels=class_names)\n",
        "    plt.title(title)\n",
        "    plt.ylabel('True Label')\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "    print(f\"   ‚úÖ Confusion matrix saved: {save_path}\")\n",
        "\n",
        "# ============================================================================\n",
        "# MAIN EXECUTION\n",
        "# ============================================================================\n",
        "if __name__ == \"__main__\":\n",
        "    model, history = train_phase3_lwf()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
