{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rEvyqVmyHsop",
        "outputId": "74a83b41-d067-4163-f5a6-3b48d64792cd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1cIGCfx6CiVgEpq8PyKzmF1LBJiQGkxzc\n",
            "From (redirected): https://drive.google.com/uc?id=1cIGCfx6CiVgEpq8PyKzmF1LBJiQGkxzc&confirm=t&uuid=ad27cd07-e175-4b85-aa80-5586c716b440\n",
            "To: /content/OCT2017.tar.gz\n",
            "100% 5.79G/5.79G [01:16<00:00, 75.5MB/s]\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1JobiELb-4mO_Gk3NY6eyIz-3oRw3U2zT\n",
            "From (redirected): https://drive.google.com/uc?id=1JobiELb-4mO_Gk3NY6eyIz-3oRw3U2zT&confirm=t&uuid=1ea80a4e-3e73-4b23-bacd-621f3b24e99b\n",
            "To: /content/ChestXRay2017.zip\n",
            "100% 1.24G/1.24G [00:17<00:00, 69.6MB/s]\n"
          ]
        }
      ],
      "source": [
        "#Datasets Download\n",
        "!gdown --fuzzy \"https://drive.google.com/file/d/1cIGCfx6CiVgEpq8PyKzmF1LBJiQGkxzc/view?usp=sharing\"\n",
        "!gdown --fuzzy \"https://drive.google.com/file/d/1JobiELb-4mO_Gk3NY6eyIz-3oRw3U2zT/view?usp=sharing\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "atLX9AVSIzN9"
      },
      "outputs": [],
      "source": [
        "#Extract zip\n",
        "!tar -xzf \"/content/OCT2017.tar.gz\" -C /content/data/\n",
        "!unzip -q /content/ChestXRay2017.zip -d /content/data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mQYSD2cvJrro",
        "outputId": "348cf96b-34c9-4172-a5c4-5d7e3d54cce8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1Gm741jjzGMLXcbDSBPYpUEtcDWRtcMFl\n",
            "From (redirected): https://drive.google.com/uc?id=1Gm741jjzGMLXcbDSBPYpUEtcDWRtcMFl&confirm=t&uuid=baafdd3e-619f-4305-96c8-9065ba652646\n",
            "To: /content/best_mobilenetv3_student_feature_kd.pth\n",
            "100% 76.0M/76.0M [00:01<00:00, 39.3MB/s]\n"
          ]
        }
      ],
      "source": [
        "#Student distillied [Fearure based] model download\n",
        "!gdown --fuzzy \"https://drive.google.com/file/d/1Gm741jjzGMLXcbDSBPYpUEtcDWRtcMFl/view?usp=drive_link\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "36aC6UB2K2co"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bYN4AZ0xK89C"
      },
      "source": [
        "# **FISHER MATRIX**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "72e461b069c2465a8ae936de4c99b53f",
            "dc4f36b5c6a146be90016122248ba3a9",
            "82cc9533a8964f7fa7adc9e0cc7d4067",
            "e50463f3086d44b0810eb43cd4d6eb16",
            "4da2eb8b56de47eba1977f1f58b36c3f",
            "b1b3c59a38064340a461bf7876472cc6",
            "be201640562d41baba14a7b932edf906",
            "44e8db6832e84f38835e08e328897e53",
            "3ffcdc72f0dc4a53854f457bf29032dc",
            "04695279e8b246e2abaf60c318692906",
            "28bafff2b60946628901c8e9337df4db"
          ]
        },
        "id": "5CqGcEIIK2hq",
        "outputId": "844147f7-8cce-4de0-f204-70fb18921ff1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üöÄ Using device: cuda\n",
            "\n",
            "======================================================================\n",
            "üìö COMPUTING FISHER INFORMATION (ACADEMIC STANDARD)\n",
            "======================================================================\n",
            "\n",
            "üìÇ Loading Phase 2 model...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Model loaded\n",
            "   Test F1: N/A\n",
            "\n",
            "üìÇ Loading OCT training data...\n",
            "   Total samples: 83,484\n",
            "   Using 70% training split: 58,438 samples\n",
            "\n",
            "üßÆ Computing Fisher Information Matrix...\n",
            "   Samples: 58,438\n",
            "   Batches: 229\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "72e461b069c2465a8ae936de4c99b53f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Computing Fisher:   0%|          | 0/229 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "üîç FISHER INFORMATION STATISTICS\n",
            "======================================================================\n",
            "\n",
            "üìä OVERALL FISHER STATISTICS:\n",
            "   Parameters tracked: 178\n",
            "   Mean of means:   0.00190823\n",
            "   Std of means:    0.00538226\n",
            "   Mean of maxs:    0.031993\n",
            "   Global max:      2.002934\n",
            "\n",
            "üéØ QUALITY CHECK:\n",
            "   Target range: mean=0.0001-0.01, max=0.01-1.0\n",
            "   ‚úÖ Fisher values in reasonable range\n",
            "   ‚Üí Recommended EWC lambda: 1000-5000\n",
            "\n",
            "üìà SAMPLE STATISTICS BY LAYER TYPE:\n",
            "\n",
            "  Backbone parameters: 170\n",
            "    First layer 'backbone.features.0.0.weight':\n",
            "      Mean: 0.01350438, Max: 0.080279\n",
            "    Middle layer 'backbone.features.9.block.0.1.weight':\n",
            "      Mean: 0.00030469, Max: 0.002669\n",
            "    Last layer 'backbone.features.16.1.bias':\n",
            "      Mean: 0.00000952, Max: 0.000213\n",
            "\n",
            "  Classifier parameters: 4\n",
            "    'backbone.classifier.0.weight':\n",
            "      Mean: 0.00000144, Max: 0.000236\n",
            "    'backbone.classifier.0.bias':\n",
            "      Mean: 0.00004006, Max: 0.000221\n",
            "    'backbone.classifier.3.weight':\n",
            "      Mean: 0.01394666, Max: 0.218337\n",
            "    'backbone.classifier.3.bias':\n",
            "      Mean: 0.01685653, Max: 0.028379\n",
            "======================================================================\n",
            "\n",
            "======================================================================\n",
            "üîç FISHER KEY STRUCTURE (FOR PHASE 3 MAPPING)\n",
            "======================================================================\n",
            "\n",
            "üìã ALL FISHER PARAMETER KEYS (178 total):\n",
            "Format: 'layer_path.weight/bias' -> shape\n",
            "\n",
            "üìä SUMMARY FOR PHASE 3:\n",
            "  Backbone parameters (features): 0\n",
            "  Classifier parameters: 0\n",
            "\n",
            "üéØ CLASSIFIER LAYER INDICES (CRITICAL FOR PHASE 3):\n",
            "======================================================================\n",
            "\n",
            "üíæ Saving Fisher Information for Phase 3...\n",
            "‚úÖ Saved to: /content/fisher/fisher_phase2.pth\n",
            "\n",
            "üéØ FOR PHASE 3:\n",
            "   Backbone params: Look for keys starting with 'backbone.features'\n",
            "   Classifier params: Map 'backbone.classifier.x' to 'head_a.x'\n",
            "   Layer 0 -> head_a.0 (Linear 960->256)\n",
            "   Layer 3 -> head_a.3 (Linear 256->4)\n",
            "\n",
            "======================================================================\n",
            "‚úÖ FISHER COMPUTATION COMPLETE - READY FOR PHASE 3\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "# =================================================================================\n",
        "# FISHER INFORMATION COMPUTATION [Kirkpatrick et al. (2017)]\n",
        "# =================================================================================\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "from pathlib import Path\n",
        "from tqdm.auto import tqdm\n",
        "from PIL import Image\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "# =================================================================================\n",
        "# CONFIGURATION\n",
        "# =================================================================================\n",
        "class FisherConfig:\n",
        "    PHASE2_MODEL_PATH = '/content/best_mobilenetv3_student_feature_kd.pth'\n",
        "    OCT_DATA_PATH = '/content/data/OCT2017'\n",
        "    OUTPUT_PATH = '/content/fisher/fisher_phase2.pth'\n",
        "\n",
        "    BATCH_SIZE = 256  # Larger batch for stable gradients\n",
        "    NUM_WORKERS = 2\n",
        "    RANDOM_SEED = 42  #Same seed as Phase 2 training\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "print(f\" Using device: {FisherConfig.device}\")\n",
        "\n",
        "# =================================================================================\n",
        "# DATASET CLASS\n",
        "# =================================================================================\n",
        "class MultiClassOCTDataset(Dataset):\n",
        "    def __init__(self, image_paths, labels, class_names, transform=None):\n",
        "        self.image_paths = image_paths\n",
        "        self.labels = labels\n",
        "        self.class_names = class_names\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = Image.open(self.image_paths[idx]).convert('RGB')\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, self.labels[idx]\n",
        "\n",
        "# =================================================================================\n",
        "# DATA LOADING\n",
        "# =================================================================================\n",
        "def load_oct_paths(root_dir, split='train'):\n",
        "    \"\"\"Load OCT image paths and labels\"\"\"\n",
        "    root_dir = Path(root_dir) / split\n",
        "    class_names = ['CNV', 'DME', 'DRUSEN', 'NORMAL']\n",
        "    image_paths, labels = [], []\n",
        "\n",
        "    for class_idx, class_name in enumerate(class_names):\n",
        "        class_dir = root_dir / class_name\n",
        "        if class_dir.exists():\n",
        "            img_files = list(class_dir.glob('*.jpeg')) + list(class_dir.glob('*.jpg'))\n",
        "            image_paths.extend([str(f) for f in img_files])\n",
        "            labels.extend([class_idx] * len(img_files))\n",
        "\n",
        "    return image_paths, labels, class_names\n",
        "\n",
        "def create_training_dataloader():\n",
        "    \"\"\"Create training dataloader with 70% split (same as Phase 2)\"\"\"\n",
        "    print(\"\\n Loading OCT training data...\")\n",
        "\n",
        "    # Load all data\n",
        "    all_paths, all_labels, class_names = load_oct_paths(FisherConfig.OCT_DATA_PATH, 'train')\n",
        "    print(f\"   Total samples: {len(all_paths):,}\")\n",
        "\n",
        "    # ‚úÖ Same 70% split as Phase 2 training\n",
        "    train_paths, temp_paths, train_labels, temp_labels = train_test_split(\n",
        "        all_paths, all_labels,\n",
        "        test_size=0.30,\n",
        "        stratify=all_labels,\n",
        "        random_state=FisherConfig.RANDOM_SEED  # CRITICAL: Same seed!\n",
        "    )\n",
        "\n",
        "    print(f\"Using 70% training split: {len(train_paths):,} samples\")\n",
        "\n",
        "    # Data transforms (same as training)\n",
        "    train_transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    train_dataset = MultiClassOCTDataset(train_paths, train_labels, class_names, train_transform)\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=FisherConfig.BATCH_SIZE,\n",
        "        shuffle=False,  \n",
        "        num_workers=FisherConfig.NUM_WORKERS,\n",
        "        pin_memory=True\n",
        "    )\n",
        "\n",
        "    return train_loader, class_names\n",
        "\n",
        "# =================================================================================\n",
        "# MODEL DEFINITION\n",
        "# =================================================================================\n",
        "class MobileNetV3Student(nn.Module):\n",
        "    def __init__(self, num_classes=4, pretrained=True):\n",
        "        super().__init__()\n",
        "        self.backbone = models.mobilenet_v3_large(pretrained=pretrained)\n",
        "        in_features = self.backbone.classifier[0].in_features\n",
        "        self.backbone.classifier = nn.Sequential(\n",
        "            nn.Linear(in_features, 256),\n",
        "            nn.Hardswish(inplace=True),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(256, num_classes)\n",
        "        )\n",
        "\n",
        "        # (from Phase 2)\n",
        "        self.feature_dim = in_features\n",
        "        self.feature_projector = nn.Sequential(\n",
        "            nn.Linear(self.feature_dim, 1024),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(1024, 2048)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.backbone(x)\n",
        "\n",
        "def load_phase2_model():\n",
        "    \"\"\"Load trained Phase 2 model\"\"\"\n",
        "    print(\"\\n Loading Phase 2 model...\")\n",
        "    checkpoint = torch.load(FisherConfig.PHASE2_MODEL_PATH,\n",
        "                           map_location=FisherConfig.device,\n",
        "                           weights_only=True)\n",
        "\n",
        "    student = MobileNetV3Student(num_classes=4, pretrained=False).to(FisherConfig.device)\n",
        "    student.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "    print(f\" Model loaded\")\n",
        "    print(f\"   Test F1: {checkpoint.get('test_f1', 'N/A')}\")\n",
        "\n",
        "    return student, checkpoint\n",
        "\n",
        "# =================================================================================\n",
        "# FISHER INFORMATION COMPUTATION\n",
        "# =================================================================================\n",
        "def compute_fisher_information(model, dataloader):\n",
        "    \"\"\"\n",
        "    Compute Fisher Information Matrix\n",
        "    Based on Kirkpatrick et al. (2017) - \"Overcoming Catastrophic Forgetting\"\n",
        "\n",
        "    \"\"\"\n",
        "    print(f\"\\nüßÆ Computing Fisher Information Matrix...\")\n",
        "    print(f\"   Samples: {len(dataloader.dataset):,}\")\n",
        "    print(f\"   Batches: {len(dataloader)}\")\n",
        "\n",
        "    \n",
        "    model.train()\n",
        "\n",
        "    fisher = {}\n",
        "\n",
        "    # Initialize Fisher dictionary\n",
        "    for name, param in model.named_parameters():\n",
        "        if param.requires_grad:\n",
        "            fisher[name] = torch.zeros_like(param, device='cpu')\n",
        "\n",
        "    samples_processed = 0\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    pbar = tqdm(dataloader, desc=\"Computing Fisher\", total=len(dataloader))\n",
        "\n",
        "    for inputs, labels in pbar:\n",
        "        inputs, labels = inputs.to(FisherConfig.device), labels.to(FisherConfig.device)\n",
        "\n",
        "        model.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "\n",
        "        #  Accumulate squared gradients (no batch size multiplication)\n",
        "        for name, param in model.named_parameters():\n",
        "            if param.requires_grad and param.grad is not None:\n",
        "                fisher[name] += param.grad.detach().cpu().pow(2)\n",
        "\n",
        "        samples_processed += inputs.size(0)\n",
        "        pbar.set_postfix({'samples': samples_processed})\n",
        "\n",
        "        del outputs, loss\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    #Normalize \n",
        "    num_batches = len(dataloader)\n",
        "    for name in fisher:\n",
        "        pass\n",
        "\n",
        "    return fisher\n",
        "\n",
        "def analyze_fisher_statistics(fisher_dict):\n",
        "    \"\"\"Analyze and print Fisher Information statistics\"\"\"\n",
        "    print(f\"\\n\" + \"=\"*70)\n",
        "    print(\" FISHER INFORMATION STATISTICS\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # Compute statistics for each parameter\n",
        "    fisher_stats = {}\n",
        "    for key, value in fisher_dict.items():\n",
        "        fisher_stats[key] = {\n",
        "            'mean': value.mean().item(),\n",
        "            'std': value.std().item(),\n",
        "            'min': value.min().item(),\n",
        "            'max': value.max().item(),\n",
        "            'median': value.median().item()\n",
        "        }\n",
        "\n",
        "    # Overall statistics\n",
        "    all_means = [s['mean'] for s in fisher_stats.values()]\n",
        "    all_maxs = [s['max'] for s in fisher_stats.values()]\n",
        "    all_stds = [s['std'] for s in fisher_stats.values()]\n",
        "\n",
        "    print(f\"\\nüìä OVERALL FISHER STATISTICS:\")\n",
        "    print(f\"   Parameters tracked: {len(fisher_dict)}\")\n",
        "    print(f\"   Mean of means:   {np.mean(all_means):.8f}\")\n",
        "    print(f\"   Std of means:    {np.std(all_means):.8f}\")\n",
        "    print(f\"   Mean of maxs:    {np.mean(all_maxs):.6f}\")\n",
        "    print(f\"   Global max:      {max(all_maxs):.6f}\")\n",
        "\n",
        "    # Quality check\n",
        "    mean_of_means = np.mean(all_means)\n",
        "    print(f\"\\n QUALITY CHECK:\")\n",
        "    print(f\"   Target range: mean=0.0001-0.01, max=0.01-1.0\")\n",
        "\n",
        "    if mean_of_means < 0.00001:\n",
        "        print(f\"    WARNING: Fisher values very small (mean={mean_of_means:.10f})\")\n",
        "        print(f\"   ‚Üí This may require higher EWC lambda (>10000)\")\n",
        "    elif mean_of_means > 0.1:\n",
        "        print(f\"   WARNING: Fisher values very large (mean={mean_of_means:.6f})\")\n",
        "        print(f\"   ‚Üí This may require lower EWC lambda (<1000)\")\n",
        "    else:\n",
        "        print(f\"   Fisher values in reasonable range\")\n",
        "        print(f\"   ‚Üí Recommended EWC lambda: 1000-5000\")\n",
        "\n",
        "    # Sample statistics by layer type\n",
        "    print(f\"\\nüìà SAMPLE STATISTICS BY LAYER TYPE:\")\n",
        "\n",
        "    backbone_keys = [k for k in fisher_dict.keys() if 'features' in k]\n",
        "    classifier_keys = [k for k in fisher_dict.keys() if 'classifier' in k]\n",
        "\n",
        "    print(f\"\\n  Backbone parameters: {len(backbone_keys)}\")\n",
        "    if backbone_keys:\n",
        "        # First, middle, last backbone layers\n",
        "        for desc, idx in [('First', 0), ('Middle', len(backbone_keys)//2), ('Last', -1)]:\n",
        "            key = backbone_keys[idx]\n",
        "            stats = fisher_stats[key]\n",
        "            print(f\"    {desc} layer '{key}':\")\n",
        "            print(f\"      Mean: {stats['mean']:.8f}, Max: {stats['max']:.6f}\")\n",
        "\n",
        "    print(f\"\\n  Classifier parameters: {len(classifier_keys)}\")\n",
        "    if classifier_keys:\n",
        "        for key in classifier_keys:\n",
        "            stats = fisher_stats[key]\n",
        "            print(f\"    '{key}':\")\n",
        "            print(f\"      Mean: {stats['mean']:.8f}, Max: {stats['max']:.6f}\")\n",
        "\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    return fisher_stats\n",
        "\n",
        "def print_fisher_structure(fisher_dict):\n",
        "    \"\"\"Print Fisher parameter structure for Phase 3 mapping\"\"\"\n",
        "    print(f\"\\n\" + \"=\"*70)\n",
        "    print(\"üîç FISHER KEY STRUCTURE (FOR PHASE 3 MAPPING)\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    print(f\"\\nüìã ALL FISHER PARAMETER KEYS ({len(fisher_dict)} total):\")\n",
        "    print(\"Format: 'layer_path.weight/bias' -> shape\")\n",
        "\n",
        "    backbone_keys = []\n",
        "    classifier_keys = []\n",
        "\n",
        "    # for key in sorted(fisher_dict.keys()):\n",
        "    #     print(f\"  {key} -> shape: {fisher_dict[key].shape}\")\n",
        "    #     if 'features' in key:\n",
        "    #         backbone_keys.append(key)\n",
        "    #     elif 'classifier' in key:\n",
        "    #         classifier_keys.append(key)\n",
        "\n",
        "    print(f\"\\nüìä SUMMARY FOR PHASE 3:\")\n",
        "    print(f\"  Backbone parameters (features): {len(backbone_keys)}\")\n",
        "    print(f\"  Classifier parameters: {len(classifier_keys)}\")\n",
        "\n",
        "    print(f\"\\nüéØ CLASSIFIER LAYER INDICES (CRITICAL FOR PHASE 3):\")\n",
        "    for key in classifier_keys:\n",
        "        parts = key.split('.')\n",
        "        layer_idx = parts[2] if len(parts) > 2 else \"?\"\n",
        "        param_type = parts[3] if len(parts) > 3 else \"?\"\n",
        "        #print(f\"  {key}\")\n",
        "        #print(f\"    -> Layer index: {layer_idx}, Type: {param_type}, Shape: {fisher_dict[key].shape}\")\n",
        "\n",
        "    print(\"=\"*70)\n",
        "\n",
        "# =================================================================================\n",
        "# SAVE FISHER INFORMATION\n",
        "# =================================================================================\n",
        "def save_fisher_checkpoint(model, fisher_dict, fisher_stats, class_names, checkpoint):\n",
        "    \"\"\"Save Fisher information with Phase 3 compatibility\"\"\"\n",
        "    print(\"\\nüíæ Saving Fisher Information for Phase 3...\")\n",
        "\n",
        "    # Phase 3 mapping information\n",
        "    phase3_mapping_info = {\n",
        "        'backbone_prefix': 'backbone.features',\n",
        "        'classifier_prefix': 'backbone.classifier',\n",
        "        'classifier_layers': {\n",
        "            '0': {'type': 'Linear', 'in_features': 960, 'out_features': 256},\n",
        "            '3': {'type': 'Linear', 'in_features': 256, 'out_features': 4}\n",
        "        }\n",
        "    }\n",
        "\n",
        "    save_checkpoint = {\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'fisher_information': fisher_dict,\n",
        "        'fisher_statistics': fisher_stats,\n",
        "        'phase3_mapping': phase3_mapping_info,\n",
        "        'class_names': class_names,\n",
        "        'test_f1': checkpoint.get('test_f1', 'N/A'),\n",
        "        'computation_method': 'academic_standard_train_mode'\n",
        "    }\n",
        "\n",
        "    torch.save(save_checkpoint, FisherConfig.OUTPUT_PATH)\n",
        "\n",
        "    print(f\"Saved to: {FisherConfig.OUTPUT_PATH}\")\n",
        "    print(f\"\\nüéØ FOR PHASE 3:\")\n",
        "    print(f\"   Backbone params: Look for keys starting with 'backbone.features'\")\n",
        "    print(f\"   Classifier params: Map 'backbone.classifier.x' to 'head_a.x'\")\n",
        "    print(f\"   Layer 0 -> head_a.0 (Linear 960->256)\")\n",
        "    print(f\"   Layer 3 -> head_a.3 (Linear 256->4)\")\n",
        "\n",
        "# =================================================================================\n",
        "# MAIN EXECUTION\n",
        "# =================================================================================\n",
        "def main():\n",
        "    \"\"\"Main execution flow\"\"\"\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"üìö COMPUTING FISHER INFORMATION (ACADEMIC STANDARD)\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # Step 1: Load Phase 2 model\n",
        "    model, checkpoint = load_phase2_model()\n",
        "\n",
        "    # Step 2: Create training dataloader (70% split)\n",
        "    train_loader, class_names = create_training_dataloader()\n",
        "\n",
        "    # Step 3: Compute Fisher Information\n",
        "    fisher_dict = compute_fisher_information(model, train_loader)\n",
        "\n",
        "    # Step 4: Analyze Fisher statistics\n",
        "    fisher_stats = analyze_fisher_statistics(fisher_dict)\n",
        "\n",
        "    # Step 5: Print structure for Phase 3 mapping\n",
        "    print_fisher_structure(fisher_dict)\n",
        "\n",
        "    # Step 6: Save everything\n",
        "    save_fisher_checkpoint(model, fisher_dict, fisher_stats, class_names, checkpoint)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"‚úÖ FISHER COMPUTATION COMPLETE - READY FOR PHASE 3\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "# =================================================================================\n",
        "# RUN\n",
        "# =================================================================================\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rRCEdSQMK2kG"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yXPV_AjRrU3q"
      },
      "source": [
        "**Continual Learning using EWC**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pCM68gV9Ye05",
        "outputId": "86ad2d27-10ab-400d-e380-b028ab74c444"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "üöÄ PHASE 3: CONTINUAL LEARNING WITH EWC\n",
            "======================================================================\n",
            "\n",
            "üìÇ Loading Phase 2 assets...\n",
            "   ‚úÖ Phase 2 model loaded\n",
            "   ‚úÖ Fisher information loaded (178 parameters)\n",
            "   ‚úÖ Optimal parameters loaded (178 parameters)\n",
            "\n",
            "   üîç Model Structure Verification:\n",
            "      features: 2,971,952 params\n",
            "      head_a: 247,044 params\n",
            "      head_b: 246,530 params\n",
            "      feature_projector: 3,083,264 params\n",
            "\n",
            "üìä Creating Task A (OCT) evaluation splits...\n",
            "   Total Task A samples: 83,484\n",
            "   Classes: ['CNV', 'DME', 'DRUSEN', 'NORMAL']\n",
            "   Class distribution: {0: 37205, 1: 11348, 2: 8616, 3: 26315}\n",
            "   Train: 58,438 | Val: 12,523 | Test: 12,523\n",
            "\n",
            "üß™ Evaluating Task A (OCT) BEFORE fine-tuning...\n",
            "   Task A Accuracy: 96.92%\n",
            "   Task A F1: 0.9693\n",
            "\n",
            "üìÇ Creating Task B (Chest X-ray) splits...\n",
            "   Total samples: 5,232\n",
            "   Classes: ['NORMAL', 'PNEUMONIA']\n",
            "   Class distribution: {0: 1349, 1: 3883}\n",
            "   Train: 3,662 | Val: 785 | Test: 785\n",
            "   Class weights: [1.9396186 0.6736571]\n",
            "\n",
            "üéØ Training Task B (Chest X-ray) with EWC (Œª=5000)...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/15: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 29/29 [01:02<00:00,  2.16s/it, loss=0.2752, ce=0.2340, ewc=0.0413]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "   Epoch 1 - Task B Val F1: 0.6652 | Acc: 75.41%\n",
            "   üíæ Best model saved (Val F1: 0.6652)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/15: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 29/29 [01:00<00:00,  2.09s/it, loss=0.2004, ce=0.1586, ewc=0.0417]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "   Epoch 2 - Task B Val F1: 0.6809 | Acc: 76.43%\n",
            "   üìà Task A Retention: F1=0.9181 (94.72% of baseline)\n",
            "   üíæ Best model saved (Val F1: 0.6809)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3/15: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 29/29 [01:00<00:00,  2.10s/it, loss=0.2265, ce=0.1867, ewc=0.0398]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "   Epoch 3 - Task B Val F1: 0.7362 | Acc: 79.24%\n",
            "   üíæ Best model saved (Val F1: 0.7362)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4/15: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 29/29 [01:00<00:00,  2.09s/it, loss=0.2235, ce=0.1849, ewc=0.0386]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "   Epoch 4 - Task B Val F1: 0.8464 | Acc: 86.37%\n",
            "   üìà Task A Retention: F1=0.8014 (82.68% of baseline)\n",
            "   üíæ Best model saved (Val F1: 0.8464)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5/15: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 29/29 [00:59<00:00,  2.06s/it, loss=0.1591, ce=0.1211, ewc=0.0380]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "   Epoch 5 - Task B Val F1: 0.9106 | Acc: 91.59%\n",
            "   üíæ Best model saved (Val F1: 0.9106)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 6/15: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 29/29 [01:01<00:00,  2.11s/it, loss=0.0656, ce=0.0260, ewc=0.0396]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "   Epoch 6 - Task B Val F1: 0.9519 | Acc: 95.29%\n",
            "   üìà Task A Retention: F1=0.6448 (66.53% of baseline)\n",
            "   üíæ Best model saved (Val F1: 0.9519)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 7/15: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 29/29 [01:00<00:00,  2.10s/it, loss=0.1261, ce=0.0862, ewc=0.0400]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "   Epoch 7 - Task B Val F1: 0.9626 | Acc: 96.31%\n",
            "   üíæ Best model saved (Val F1: 0.9626)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 8/15: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 29/29 [01:00<00:00,  2.09s/it, loss=0.1836, ce=0.1450, ewc=0.0386]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "   Epoch 8 - Task B Val F1: 0.9730 | Acc: 97.32%\n",
            "   üìà Task A Retention: F1=0.4405 (45.44% of baseline)\n",
            "   üíæ Best model saved (Val F1: 0.9730)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 9/15: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 29/29 [01:00<00:00,  2.09s/it, loss=0.1616, ce=0.1249, ewc=0.0366]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "   Epoch 9 - Task B Val F1: 0.9730 | Acc: 97.32%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 10/15: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 29/29 [01:00<00:00,  2.08s/it, loss=0.0955, ce=0.0571, ewc=0.0383]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "   Epoch 10 - Task B Val F1: 0.9650 | Acc: 96.56%\n",
            "   üìà Task A Retention: F1=0.4194 (43.26% of baseline)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 11/15: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 29/29 [01:01<00:00,  2.13s/it, loss=0.1580, ce=0.1180, ewc=0.0400]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "   Epoch 11 - Task B Val F1: 0.9649 | Acc: 96.56%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 12/15: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 29/29 [01:01<00:00,  2.11s/it, loss=0.1769, ce=0.1394, ewc=0.0375]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "   Epoch 12 - Task B Val F1: 0.9809 | Acc: 98.09%\n",
            "   üìà Task A Retention: F1=0.3966 (40.92% of baseline)\n",
            "   üíæ Best model saved (Val F1: 0.9809)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 13/15: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 29/29 [01:00<00:00,  2.09s/it, loss=0.0762, ce=0.0380, ewc=0.0382]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "   Epoch 13 - Task B Val F1: 0.9835 | Acc: 98.34%\n",
            "   üíæ Best model saved (Val F1: 0.9835)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 14/15: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 29/29 [01:00<00:00,  2.10s/it, loss=0.0731, ce=0.0343, ewc=0.0389]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "   Epoch 14 - Task B Val F1: 0.9673 | Acc: 96.69%\n",
            "   üìà Task A Retention: F1=0.3659 (37.75% of baseline)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 15/15: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 29/29 [00:59<00:00,  2.06s/it, loss=0.0778, ce=0.0408, ewc=0.0370]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "   Epoch 15 - Task B Val F1: 0.9834 | Acc: 98.34%\n",
            "\n",
            "======================================================================\n",
            "üìä FINAL EVALUATION\n",
            "======================================================================\n",
            "\n",
            "======================================================================\n",
            "üìä TASK B (Chest X-ray) EVALUATION\n",
            "======================================================================\n",
            "   Accuracy:  97.58%\n",
            "   F1-Score:  0.9758\n",
            "\n",
            "üìã Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      NORMAL     0.9510    0.9557    0.9533       203\n",
            "   PNEUMONIA     0.9845    0.9828    0.9837       582\n",
            "\n",
            "    accuracy                         0.9758       785\n",
            "   macro avg     0.9677    0.9692    0.9685       785\n",
            "weighted avg     0.9758    0.9758    0.9758       785\n",
            "\n",
            "\n",
            "======================================================================\n",
            "üìä TASK A (OCT) - Retention Check EVALUATION\n",
            "======================================================================\n",
            "   Accuracy:  47.33%\n",
            "   F1-Score:  0.3910\n",
            "\n",
            "üìã Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         CNV     0.9934    0.1885    0.3169      5581\n",
            "         DME     0.5243    0.0317    0.0598      1702\n",
            "      DRUSEN     0.4176    0.6783    0.5169      1293\n",
            "      NORMAL     0.4259    0.9992    0.5972      3947\n",
            "\n",
            "    accuracy                         0.4733     12523\n",
            "   macro avg     0.5903    0.4744    0.3727     12523\n",
            "weighted avg     0.6913    0.4733    0.3910     12523\n",
            "\n",
            "\n",
            "======================================================================\n",
            "üéØ CONTINUAL LEARNING SUMMARY\n",
            "======================================================================\n",
            "üìä Task A (OCT) Retention:\n",
            "   Before: F1=0.9693, Acc=96.92%\n",
            "   After:  F1=0.3910, Acc=47.33%\n",
            "   Retention: F1=40.33%, Acc=48.83%\n",
            "\n",
            "üìä Task B (Chest X-ray) Performance:\n",
            "   Test F1: 0.9758\n",
            "   Test Acc: 97.58%\n",
            "======================================================================\n",
            "   ‚úÖ Confusion matrix saved: /content/phase3_results/cm_task_a.png\n",
            "   ‚úÖ Confusion matrix saved: /content/phase3_results/cm_task_b.png\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "import torchvision.models as models\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, f1_score, accuracy_score, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from collections import Counter\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "\n",
        "# ============================================================================\n",
        "# CONFIGURATION\n",
        "# ============================================================================\n",
        "class Config:\n",
        "    # Paths\n",
        "    TASK_A_DATA_PATH = \"/content/data/OCT2017/train\"  # OCT images folder\n",
        "    TASK_B_DATA_PATH = \"/content/data/chest_xray/train\"  # Chest X-ray images folder\n",
        "    PHASE2_MODEL_PATH = \"/content/best_mobilenetv3_student_kd.pth\"\n",
        "    FISHER_PATH = \"/content/fisher/fisher_phase2.pth\"\n",
        "    SAVE_DIR = \"/content/phase3_results\"\n",
        "\n",
        "    # Model settings\n",
        "    TASK_A_CLASSES = 4  # OCT classes\n",
        "    TASK_B_CLASSES = 2  # Chest X-ray classes\n",
        "\n",
        "    # EWC hyperparameters\n",
        "    EWC_LAMBDA = 5000  # EWC regularization strength\n",
        "\n",
        "    # Training hyperparameters\n",
        "    BATCH_SIZE = 128\n",
        "    LEARNING_RATE = 0.0001\n",
        "    NUM_EPOCHS = 15\n",
        "    PATIENCE = 5  # Early stopping\n",
        "\n",
        "    # Data augmentation\n",
        "    USE_AUGMENTATION = True\n",
        "\n",
        "    # Evaluation\n",
        "    EVAL_TASK_A_EVERY = 2  # Evaluate Task A retention every N epochs\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# ============================================================================\n",
        "# DATA LOADING UTILITIES\n",
        "# ============================================================================\n",
        "def load_task_paths(data_path):\n",
        "    \"\"\"\n",
        "    Universal data loader for both OCT and Chest X-ray\n",
        "    Loads from: /path/to/train/CLASS_NAME/*.jpg\n",
        "    \"\"\"\n",
        "    data_path = Path(data_path)\n",
        "\n",
        "    # Get all class folders\n",
        "    class_names = sorted([d.name for d in data_path.iterdir() if d.is_dir()])\n",
        "\n",
        "    all_paths = []\n",
        "    all_labels = []\n",
        "\n",
        "    for idx, class_name in enumerate(class_names):\n",
        "        class_dir = data_path / class_name\n",
        "        # Support multiple image formats\n",
        "        paths = list(class_dir.glob('*.jpeg')) + \\\n",
        "                list(class_dir.glob('*.jpg')) + \\\n",
        "                list(class_dir.glob('*.png'))\n",
        "\n",
        "        all_paths.extend(paths)\n",
        "        all_labels.extend([idx] * len(paths))\n",
        "\n",
        "    return all_paths, all_labels, class_names\n",
        "\n",
        "# ============================================================================\n",
        "# DATASET CLASS\n",
        "# ============================================================================\n",
        "class ImageDataset(Dataset):\n",
        "    def __init__(self, paths, labels, transform=None):\n",
        "        self.paths = paths\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        from PIL import Image\n",
        "        img = Image.open(self.paths[idx]).convert('RGB')\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        return img, label\n",
        "\n",
        "# ============================================================================\n",
        "# DATA SPLITS\n",
        "# ============================================================================\n",
        "def create_task_a_splits():\n",
        "    \"\"\"Create stratified splits for Task A (OCT)\"\"\"\n",
        "    print(\"\\nüìä Creating Task A (OCT) evaluation splits...\")\n",
        "\n",
        "    all_paths, all_labels, task_a_class_names = load_task_paths(Config.TASK_A_DATA_PATH)\n",
        "    print(f\"   Total Task A samples: {len(all_paths):,}\")\n",
        "    print(f\"   Classes: {task_a_class_names}\")\n",
        "\n",
        "    # Class distribution\n",
        "    class_counts = Counter(all_labels)\n",
        "    print(f\"   Class distribution: {dict(class_counts)}\")\n",
        "\n",
        "    # 70/15/15 split\n",
        "    train_paths, temp_paths, train_labels, temp_labels = train_test_split(\n",
        "        all_paths, all_labels, test_size=0.30, stratify=all_labels, random_state=42\n",
        "    )\n",
        "\n",
        "    val_paths, test_paths, val_labels, test_labels = train_test_split(\n",
        "        temp_paths, temp_labels, test_size=0.50, stratify=temp_labels, random_state=42\n",
        "    )\n",
        "\n",
        "    print(f\"   Train: {len(train_paths):,} | Val: {len(val_paths):,} | Test: {len(test_paths):,}\")\n",
        "\n",
        "    test_transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    test_dataset = ImageDataset(test_paths, test_labels, test_transform)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=Config.BATCH_SIZE,\n",
        "                            shuffle=False, num_workers=2)\n",
        "\n",
        "    return test_loader, task_a_class_names\n",
        "\n",
        "def create_task_b_splits():\n",
        "    \"\"\"Create stratified splits for Task B (Chest X-ray)\"\"\"\n",
        "    print(\"\\nüìÇ Creating Task B (Chest X-ray) splits...\")\n",
        "\n",
        "    all_paths, all_labels, task_b_class_names = load_task_paths(Config.TASK_B_DATA_PATH)\n",
        "    print(f\"   Total samples: {len(all_paths):,}\")\n",
        "    print(f\"   Classes: {task_b_class_names}\")\n",
        "\n",
        "    # Class distribution\n",
        "    class_counts = Counter(all_labels)\n",
        "    print(f\"   Class distribution: {dict(class_counts)}\")\n",
        "\n",
        "    # 70/15/15 stratified split\n",
        "    train_paths, temp_paths, train_labels, temp_labels = train_test_split(\n",
        "        all_paths, all_labels, test_size=0.30, stratify=all_labels, random_state=42\n",
        "    )\n",
        "\n",
        "    val_paths, test_paths, val_labels, test_labels = train_test_split(\n",
        "        temp_paths, temp_labels, test_size=0.50, stratify=temp_labels, random_state=42\n",
        "    )\n",
        "\n",
        "    print(f\"   Train: {len(train_paths):,} | Val: {len(val_paths):,} | Test: {len(test_paths):,}\")\n",
        "\n",
        "    # Compute class weights for imbalanced dataset\n",
        "    train_class_counts = Counter(train_labels)\n",
        "    total_samples = len(train_labels)\n",
        "    class_weights = torch.tensor([\n",
        "        total_samples / (len(train_class_counts) * train_class_counts[i])\n",
        "        for i in range(len(task_b_class_names))\n",
        "    ], dtype=torch.float32).to(Config.device)\n",
        "\n",
        "    print(f\"   Class weights: {class_weights.cpu().numpy()}\")\n",
        "\n",
        "    # Data transforms with augmentation\n",
        "    if Config.USE_AUGMENTATION:\n",
        "        train_transform = transforms.Compose([\n",
        "            transforms.Resize((224, 224)),\n",
        "            transforms.RandomHorizontalFlip(p=0.5),\n",
        "            transforms.RandomRotation(10),\n",
        "            transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "        ])\n",
        "    else:\n",
        "        train_transform = transforms.Compose([\n",
        "            transforms.Resize((224, 224)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "        ])\n",
        "\n",
        "    val_transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    train_dataset = ImageDataset(train_paths, train_labels, train_transform)\n",
        "    val_dataset = ImageDataset(val_paths, val_labels, val_transform)\n",
        "    test_dataset = ImageDataset(test_paths, test_labels, val_transform)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=Config.BATCH_SIZE,\n",
        "                             shuffle=True, num_workers=2)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=Config.BATCH_SIZE,\n",
        "                           shuffle=False, num_workers=2)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=Config.BATCH_SIZE,\n",
        "                            shuffle=False, num_workers=2)\n",
        "\n",
        "    return train_loader, val_loader, test_loader, class_weights, task_b_class_names\n",
        "\n",
        "# ============================================================================\n",
        "# MULTI-HEAD MODEL\n",
        "# ============================================================================\n",
        "class MultiHeadMobileNet(nn.Module):\n",
        "    def __init__(self, num_classes_a, num_classes_b):\n",
        "        super().__init__()\n",
        "        # Load MobileNetV3\n",
        "        mobilenet = models.mobilenet_v3_large(weights=None)\n",
        "        self.features = mobilenet.features  # Backbone features\n",
        "\n",
        "        # ADD THIS - from Phase 2\n",
        "        self.feature_projector = nn.Sequential(\n",
        "            nn.Linear(960, 1024),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(1024, 2048)\n",
        "        )\n",
        "\n",
        "        # Task A head (OCT)\n",
        "        self.head_a = nn.Sequential(\n",
        "            nn.Linear(960, 256),\n",
        "            nn.Hardswish(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(256, num_classes_a)\n",
        "        )\n",
        "\n",
        "        # Task B head (Chest X-ray) - new\n",
        "        self.head_b = nn.Sequential(\n",
        "            nn.Linear(960, 256),\n",
        "            nn.Hardswish(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(256, num_classes_b)\n",
        "        )\n",
        "\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.flatten = nn.Flatten()\n",
        "\n",
        "    def forward(self, x, task='b'):\n",
        "        x = self.features(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = self.flatten(x)\n",
        "\n",
        "        if task == 'a':\n",
        "            return self.head_a(x)\n",
        "        elif task == 'b':\n",
        "            return self.head_b(x)\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown task: {task}\")\n",
        "\n",
        "# ============================================================================\n",
        "# EWC LOSS\n",
        "# ============================================================================\n",
        "\n",
        "def compute_ewc_loss(model, fisher_dict, optimal_params, lambda_ewc):\n",
        "    ewc_loss = 0.0\n",
        "\n",
        "    for name, param in model.named_parameters():\n",
        "        if name in fisher_dict:  # ONLY FILTER: Check Fisher exists\n",
        "            fisher = fisher_dict[name].to(param.device)  # Device safety\n",
        "            optimal = optimal_params[name].to(param.device)  #Device safety\n",
        "            ewc_loss += (fisher * (param - optimal).pow(2)).sum()\n",
        "\n",
        "    return (lambda_ewc / 2.0) * ewc_loss\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# LOAD PHASE 2 MODEL AND FISHER\n",
        "# ============================================================================\n",
        "def load_phase2_assets():\n",
        "    \"\"\"Load Phase 2 model and Fisher information\"\"\"\n",
        "    print(\"\\n Loading Phase 2 assets...\")\n",
        "\n",
        "    # Load Fisher data (contains Phase 2 model weights)\n",
        "    fisher_data = torch.load(Config.FISHER_PATH, map_location=Config.device)\n",
        "    phase2_state = fisher_data['model_state_dict']\n",
        "\n",
        "    # Create multi-head model\n",
        "    model = MultiHeadMobileNet(Config.TASK_A_CLASSES, Config.TASK_B_CLASSES)\n",
        "\n",
        "    # Load weights with correct mapping\n",
        "    model_state = {}\n",
        "\n",
        "    for key, value in phase2_state.items():\n",
        "        if key.startswith('backbone.features'):\n",
        "            # backbone.features.X -> features.X\n",
        "            new_key = key.replace('backbone.', '')\n",
        "            model_state[new_key] = value\n",
        "        elif key.startswith('backbone.classifier'):\n",
        "            # backbone.classifier.X -> head_a.X\n",
        "            new_key = key.replace('backbone.classifier', 'head_a')\n",
        "            model_state[new_key] = value\n",
        "        elif key.startswith('feature_projector'):\n",
        "            # Keep feature_projector as-is\n",
        "            model_state[key] = value\n",
        "\n",
        "    # Load the mapped weights\n",
        "    model.load_state_dict(model_state, strict=False)\n",
        "    model = model.to(Config.device)\n",
        "    print(\"    Phase 2 model loaded\")\n",
        "\n",
        "    # Load Fisher information\n",
        "    fisher_dict = fisher_data['fisher_information']\n",
        "    optimal_params = phase2_state\n",
        "\n",
        "    # Map Fisher keys to new model structure\n",
        "    # IMPORTANT: Only iterate over Fisher keys (trainable params only)\n",
        "    mapped_fisher = {}\n",
        "    mapped_optimal = {}\n",
        "\n",
        "    for key in fisher_dict.keys():  # Fisher only has trainable parameters\n",
        "        if key.startswith('backbone.features'):\n",
        "            # backbone.features.X -> features.X\n",
        "            new_key = key.replace('backbone.', '')\n",
        "        elif key.startswith('backbone.classifier'):\n",
        "            # backbone.classifier.X -> head_a.X\n",
        "            new_key = key.replace('backbone.classifier', 'head_a')\n",
        "        elif key.startswith('feature_projector'):\n",
        "            # Keep feature_projector as-is\n",
        "            new_key = key\n",
        "        else:\n",
        "            continue\n",
        "\n",
        "        # Map both Fisher and optimal params\n",
        "        mapped_fisher[new_key] = fisher_dict[key]\n",
        "        mapped_optimal[new_key] = optimal_params[key]\n",
        "\n",
        "    print(f\"   Fisher information loaded ({len(mapped_fisher)} parameters)\")\n",
        "    print(f\"   Optimal parameters loaded ({len(mapped_optimal)} parameters)\")\n",
        "\n",
        "    # Verification\n",
        "    print(f\"\\n   Model Structure Verification:\")\n",
        "    print(f\"      features: {sum(p.numel() for p in model.features.parameters()):,} params\")\n",
        "    print(f\"      head_a: {sum(p.numel() for p in model.head_a.parameters()):,} params\")\n",
        "    print(f\"      head_b: {sum(p.numel() for p in model.head_b.parameters()):,} params\")\n",
        "    print(f\"      feature_projector: {sum(p.numel() for p in model.feature_projector.parameters()):,} params\")\n",
        "\n",
        "    return model, mapped_fisher, mapped_optimal\n",
        "\n",
        "# ============================================================================\n",
        "# EVALUATION FUNCTIONS\n",
        "# ============================================================================\n",
        "def evaluate_task(model, dataloader, task, class_names):\n",
        "    \"\"\"Evaluate model on a specific task\"\"\"\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in dataloader:\n",
        "            images = images.to(Config.device)\n",
        "            outputs = model(images, task=task)\n",
        "            preds = outputs.argmax(dim=1)\n",
        "\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.numpy())\n",
        "\n",
        "    acc = accuracy_score(all_labels, all_preds)\n",
        "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
        "\n",
        "    return acc, f1, all_preds, all_labels\n",
        "\n",
        "def print_evaluation_report(acc, f1, preds, labels, class_names, task_name):\n",
        "    \"\"\"Print detailed evaluation report\"\"\"\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\" {task_name} EVALUATION\")\n",
        "    print(f\"{'='*70}\")\n",
        "    print(f\"   Accuracy:  {acc*100:.2f}%\")\n",
        "    print(f\"   F1-Score:  {f1:.4f}\")\n",
        "    print(f\"\\n Classification Report:\")\n",
        "    print(classification_report(labels, preds, target_names=class_names, digits=4))\n",
        "\n",
        "# ============================================================================\n",
        "# TRAINING FUNCTION\n",
        "# ============================================================================\n",
        "def train_phase3():\n",
        "    \"\"\"Phase 3: Continual Learning with EWC\"\"\"\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\" PHASE 3: CONTINUAL LEARNING WITH EWC\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # Create save directory\n",
        "    Path(Config.SAVE_DIR).mkdir(exist_ok=True)\n",
        "\n",
        "    # Load Phase 2 assets\n",
        "    model, fisher_dict, optimal_params = load_phase2_assets()\n",
        "\n",
        "\n",
        "            # Freeze Task A head\n",
        "    for p in model.head_a.parameters():\n",
        "        p.requires_grad = False\n",
        "\n",
        "    # verify\n",
        "    for name, p in model.named_parameters():\n",
        "        if \"head_a\" in name:\n",
        "            assert p.requires_grad is False\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # Create Task A test loader for retention evaluation\n",
        "    task_a_test_loader, task_a_classes = create_task_a_splits()\n",
        "\n",
        "    # Evaluate Task A before fine-tuning (baseline)\n",
        "    print(\"\\nüß™ Evaluating Task A (OCT) BEFORE fine-tuning...\")\n",
        "    task_a_acc_before, task_a_f1_before, _, _ = evaluate_task(\n",
        "        model, task_a_test_loader, task='a', class_names=task_a_classes\n",
        "    )\n",
        "    print(f\"   Task A Accuracy: {task_a_acc_before*100:.2f}%\")\n",
        "    print(f\"   Task A F1: {task_a_f1_before:.4f}\")\n",
        "\n",
        "    # Create Task B dataloaders\n",
        "    train_loader, val_loader, test_loader, class_weights, task_b_classes = create_task_b_splits()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # Setup training\n",
        "    # In train_phase3 function:\n",
        "    optimizer = optim.Adam([\n",
        "      {'params': model.features.parameters(), 'lr': Config.LEARNING_RATE},\n",
        "      {'params': model.head_b.parameters(), 'lr': Config.LEARNING_RATE}\n",
        "    ])\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5,\n",
        "                                                     patience=3,)\n",
        "\n",
        "    # Training loop\n",
        "    best_val_f1 = 0.0\n",
        "    patience_counter = 0\n",
        "    history = {'train_loss': [], 'val_f1': [], 'task_a_f1': []}\n",
        "\n",
        "    print(f\"\\n Training Task B (Chest X-ray) with EWC (Œª={Config.EWC_LAMBDA})...\")\n",
        "\n",
        "    for epoch in range(Config.NUM_EPOCHS):\n",
        "        # Training\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "\n",
        "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{Config.NUM_EPOCHS}\")\n",
        "        for images, labels in pbar:\n",
        "            images, labels = images.to(Config.device), labels.to(Config.device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images, task='b')\n",
        "\n",
        "            # Task B loss (cross-entropy with class weights)\n",
        "            ce_loss = criterion(outputs, labels)\n",
        "\n",
        "            # EWC regularization loss\n",
        "            ewc_loss = compute_ewc_loss(model, fisher_dict, optimal_params, Config.EWC_LAMBDA)\n",
        "\n",
        "            # Total loss\n",
        "            total_loss = ce_loss + ewc_loss\n",
        "\n",
        "            total_loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += total_loss.item()\n",
        "            pbar.set_postfix({'loss': f'{total_loss.item():.4f}',\n",
        "                            'ce': f'{ce_loss.item():.4f}',\n",
        "                            'ewc': f'{ewc_loss.item():.4f}'})\n",
        "\n",
        "        avg_train_loss = train_loss / len(train_loader)\n",
        "        history['train_loss'].append(avg_train_loss)\n",
        "\n",
        "        # Validation on Task B\n",
        "        val_acc, val_f1, _, _ = evaluate_task(model, val_loader, task='b',\n",
        "                                             class_names=task_b_classes)\n",
        "        history['val_f1'].append(val_f1)\n",
        "\n",
        "        print(f\"\\n   Epoch {epoch+1} - Task B Val F1: {val_f1:.4f} | Acc: {val_acc*100:.2f}%\")\n",
        "\n",
        "        # Evaluate Task A retention periodically\n",
        "        if (epoch + 1) % Config.EVAL_TASK_A_EVERY == 0:\n",
        "            task_a_acc, task_a_f1, _, _ = evaluate_task(model, task_a_test_loader,\n",
        "                                                        task='a', class_names=task_a_classes)\n",
        "            history['task_a_f1'].append(task_a_f1)\n",
        "            retention = (task_a_f1 / task_a_f1_before) * 100\n",
        "            print(f\"    Task A Retention: F1={task_a_f1:.4f} ({retention:.2f}% of baseline)\")\n",
        "\n",
        "        # Learning rate scheduling\n",
        "        scheduler.step(val_f1)\n",
        "\n",
        "        # Early stopping and checkpointing\n",
        "        if val_f1 > best_val_f1:\n",
        "            best_val_f1 = val_f1\n",
        "            patience_counter = 0\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'val_f1': val_f1,\n",
        "                'task_a_f1_before': task_a_f1_before\n",
        "            }, f\"{Config.SAVE_DIR}/phase3_best.pth\")\n",
        "            print(f\" Best model saved (Val F1: {val_f1:.4f})\")\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= Config.PATIENCE:\n",
        "                print(f\"\\n  Early stopping triggered (patience={Config.PATIENCE})\")\n",
        "                break\n",
        "\n",
        "    # Final evaluation\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"FINAL EVALUATION\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # Load best model\n",
        "    checkpoint = torch.load(f\"{Config.SAVE_DIR}/phase3_best.pth\")\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "    # Task B (Chest X-ray) - Test set\n",
        "    task_b_acc, task_b_f1, task_b_preds, task_b_labels = evaluate_task(\n",
        "        model, test_loader, task='b', class_names=task_b_classes\n",
        "    )\n",
        "    print_evaluation_report(task_b_acc, task_b_f1, task_b_preds, task_b_labels,\n",
        "                          task_b_classes, \"TASK B (Chest X-ray)\")\n",
        "\n",
        "    # Task A (OCT) - Retention test\n",
        "    task_a_acc_after, task_a_f1_after, task_a_preds, task_a_labels = evaluate_task(\n",
        "        model, task_a_test_loader, task='a', class_names=task_a_classes\n",
        "    )\n",
        "    print_evaluation_report(task_a_acc_after, task_a_f1_after, task_a_preds, task_a_labels,\n",
        "                          task_a_classes, \"TASK A (OCT) - Retention Check\")\n",
        "\n",
        "    # Retention metrics\n",
        "    retention_f1 = (task_a_f1_after / task_a_f1_before) * 100\n",
        "    retention_acc = (task_a_acc_after / task_a_acc_before) * 100\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"CONTINUAL LEARNING SUMMARY\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\" Task A (OCT) Retention:\")\n",
        "    print(f\"   Before: F1={task_a_f1_before:.4f}, Acc={task_a_acc_before*100:.2f}%\")\n",
        "    print(f\"   After:  F1={task_a_f1_after:.4f}, Acc={task_a_acc_after*100:.2f}%\")\n",
        "    print(f\"   Retention: F1={retention_f1:.2f}%, Acc={retention_acc:.2f}%\")\n",
        "    print(f\"\\n Task B (Chest X-ray) Performance:\")\n",
        "    print(f\"   Test F1: {task_b_f1:.4f}\")\n",
        "    print(f\"   Test Acc: {task_b_acc*100:.2f}%\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # Save confusion matrices\n",
        "    save_confusion_matrix(task_a_labels, task_a_preds, task_a_classes,\n",
        "                         \"Task A (OCT) - After EWC\", f\"{Config.SAVE_DIR}/cm_task_a.png\")\n",
        "    save_confusion_matrix(task_b_labels, task_b_preds, task_b_classes,\n",
        "                         \"Task B (Chest X-ray)\", f\"{Config.SAVE_DIR}/cm_task_b.png\")\n",
        "\n",
        "    return model, history\n",
        "\n",
        "def save_confusion_matrix(labels, preds, class_names, title, save_path):\n",
        "    \"\"\"Save confusion matrix plot\"\"\"\n",
        "    cm = confusion_matrix(labels, preds)\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=class_names, yticklabels=class_names)\n",
        "    plt.title(title)\n",
        "    plt.ylabel('True Label')\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "    print(f\"  Confusion matrix saved: {save_path}\")\n",
        "\n",
        "# ============================================================================\n",
        "# MAIN EXECUTION\n",
        "# ============================================================================\n",
        "if __name__ == \"__main__\":\n",
        "    model, history = train_phase3()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mbzzk4apCpgY"
      },
      "source": [
        "# Adaating with **BatchNorm freezed**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LyD1K4LDCorV",
        "outputId": "d440ad3c-eb06-46b3-87ea-a731f5fd5c63"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "üöÄ PHASE 3: CONTINUAL LEARNING WITH EWC\n",
            "======================================================================\n",
            "\n",
            "üìÇ Loading Phase 2 assets...\n",
            "   ‚úÖ Phase 2 model loaded\n",
            "   ‚úÖ Fisher information loaded (178 parameters)\n",
            "   ‚úÖ Optimal parameters loaded (178 parameters)\n",
            "\n",
            "   üîç Model Structure Verification:\n",
            "      features: 2,971,952 params\n",
            "      head_a: 247,044 params\n",
            "      head_b: 246,530 params\n",
            "      feature_projector: 3,083,264 params\n",
            "\n",
            "üìä Creating Task A (OCT) evaluation splits...\n",
            "   Total Task A samples: 83,484\n",
            "   Classes: ['CNV', 'DME', 'DRUSEN', 'NORMAL']\n",
            "   Class distribution: {0: 37205, 1: 11348, 2: 8616, 3: 26315}\n",
            "   Train: 58,438 | Val: 12,523 | Test: 12,523\n",
            "\n",
            "üß™ Evaluating Task A (OCT) BEFORE fine-tuning...\n",
            "   Task A Accuracy: 96.92%\n",
            "   Task A F1: 0.9693\n",
            "\n",
            "üìÇ Creating Task B (Chest X-ray) splits...\n",
            "   Total samples: 5,232\n",
            "   Classes: ['NORMAL', 'PNEUMONIA']\n",
            "   Class distribution: {0: 1349, 1: 3883}\n",
            "   Train: 3,662 | Val: 785 | Test: 785\n",
            "   Class weights: [1.9396186 0.6736571]\n",
            "\n",
            "üéØ Training Task B (Chest X-ray) with EWC (Œª=5000)...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/15: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 29/29 [01:01<00:00,  2.13s/it, loss=0.2077, ce=0.1694, ewc=0.0383]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "   Epoch 1 - Task B Val F1: 0.9050 | Acc: 90.06%\n",
            "   üíæ Best model saved (Val F1: 0.9050)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/15: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 29/29 [01:03<00:00,  2.18s/it, loss=0.1440, ce=0.1059, ewc=0.0382]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "   Epoch 2 - Task B Val F1: 0.9441 | Acc: 94.27%\n",
            "   üìà Task A Retention: F1=0.9638 (99.44% of baseline)\n",
            "   üíæ Best model saved (Val F1: 0.9441)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3/15: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 29/29 [01:00<00:00,  2.10s/it, loss=0.1596, ce=0.1238, ewc=0.0357]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "   Epoch 3 - Task B Val F1: 0.9455 | Acc: 94.39%\n",
            "   üíæ Best model saved (Val F1: 0.9455)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4/15: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 29/29 [01:00<00:00,  2.08s/it, loss=0.1440, ce=0.1084, ewc=0.0356]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "   Epoch 4 - Task B Val F1: 0.9577 | Acc: 95.67%\n",
            "   üìà Task A Retention: F1=0.9545 (98.47% of baseline)\n",
            "   üíæ Best model saved (Val F1: 0.9577)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5/15: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 29/29 [01:01<00:00,  2.12s/it, loss=0.2259, ce=0.1921, ewc=0.0338]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "   Epoch 5 - Task B Val F1: 0.9480 | Acc: 94.65%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 6/15: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 29/29 [01:00<00:00,  2.09s/it, loss=0.1394, ce=0.1060, ewc=0.0334]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "   Epoch 6 - Task B Val F1: 0.9613 | Acc: 96.05%\n",
            "   üìà Task A Retention: F1=0.9498 (97.99% of baseline)\n",
            "   üíæ Best model saved (Val F1: 0.9613)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 7/15: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 29/29 [01:00<00:00,  2.10s/it, loss=0.1144, ce=0.0809, ewc=0.0335]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "   Epoch 7 - Task B Val F1: 0.9626 | Acc: 96.18%\n",
            "   üíæ Best model saved (Val F1: 0.9626)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 8/15: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 29/29 [01:00<00:00,  2.09s/it, loss=0.2373, ce=0.2049, ewc=0.0325]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "   Epoch 8 - Task B Val F1: 0.9733 | Acc: 97.32%\n",
            "   üìà Task A Retention: F1=0.9601 (99.06% of baseline)\n",
            "   üíæ Best model saved (Val F1: 0.9733)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 9/15: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 29/29 [01:01<00:00,  2.11s/it, loss=0.0885, ce=0.0554, ewc=0.0330]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "   Epoch 9 - Task B Val F1: 0.9732 | Acc: 97.32%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 10/15: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 29/29 [01:00<00:00,  2.10s/it, loss=0.0867, ce=0.0528, ewc=0.0339]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "   Epoch 10 - Task B Val F1: 0.9771 | Acc: 97.71%\n",
            "   üìà Task A Retention: F1=0.9410 (97.08% of baseline)\n",
            "   üíæ Best model saved (Val F1: 0.9771)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 11/15: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 29/29 [01:01<00:00,  2.13s/it, loss=0.1621, ce=0.1309, ewc=0.0312]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "   Epoch 11 - Task B Val F1: 0.9718 | Acc: 97.20%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 12/15: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 29/29 [01:00<00:00,  2.10s/it, loss=0.1891, ce=0.1573, ewc=0.0318]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "   Epoch 12 - Task B Val F1: 0.9664 | Acc: 96.69%\n",
            "   üìà Task A Retention: F1=0.9468 (97.68% of baseline)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 13/15: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 29/29 [01:01<00:00,  2.11s/it, loss=0.0524, ce=0.0196, ewc=0.0329]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "   Epoch 13 - Task B Val F1: 0.9797 | Acc: 97.96%\n",
            "   üíæ Best model saved (Val F1: 0.9797)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 14/15: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 29/29 [01:01<00:00,  2.13s/it, loss=0.2375, ce=0.2058, ewc=0.0317]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "   Epoch 14 - Task B Val F1: 0.9383 | Acc: 93.63%\n",
            "   üìà Task A Retention: F1=0.9601 (99.05% of baseline)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 15/15: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 29/29 [01:00<00:00,  2.08s/it, loss=0.1870, ce=0.1545, ewc=0.0325]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "   Epoch 15 - Task B Val F1: 0.9601 | Acc: 95.92%\n",
            "\n",
            "======================================================================\n",
            "üìä FINAL EVALUATION\n",
            "======================================================================\n",
            "\n",
            "======================================================================\n",
            "üìä TASK B (Chest X-ray) EVALUATION\n",
            "======================================================================\n",
            "   Accuracy:  97.07%\n",
            "   F1-Score:  0.9707\n",
            "\n",
            "üìã Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      NORMAL     0.9412    0.9458    0.9435       203\n",
            "   PNEUMONIA     0.9811    0.9794    0.9802       582\n",
            "\n",
            "    accuracy                         0.9707       785\n",
            "   macro avg     0.9611    0.9626    0.9619       785\n",
            "weighted avg     0.9708    0.9707    0.9707       785\n",
            "\n",
            "\n",
            "======================================================================\n",
            "üìä TASK A (OCT) - Retention Check EVALUATION\n",
            "======================================================================\n",
            "   Accuracy:  95.38%\n",
            "   F1-Score:  0.9527\n",
            "\n",
            "üìã Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         CNV     0.9688    0.9780    0.9733      5581\n",
            "         DME     0.9160    0.9424    0.9290      1702\n",
            "      DRUSEN     0.9522    0.7703    0.8516      1293\n",
            "      NORMAL     0.9497    0.9845    0.9668      3947\n",
            "\n",
            "    accuracy                         0.9538     12523\n",
            "   macro avg     0.9467    0.9188    0.9302     12523\n",
            "weighted avg     0.9539    0.9538    0.9527     12523\n",
            "\n",
            "\n",
            "======================================================================\n",
            "üéØ CONTINUAL LEARNING SUMMARY\n",
            "======================================================================\n",
            "üìä Task A (OCT) Retention:\n",
            "   Before: F1=0.9693, Acc=96.92%\n",
            "   After:  F1=0.9527, Acc=95.38%\n",
            "   Retention: F1=98.29%, Acc=98.41%\n",
            "\n",
            "üìä Task B (Chest X-ray) Performance:\n",
            "   Test F1: 0.9707\n",
            "   Test Acc: 97.07%\n",
            "======================================================================\n",
            "   ‚úÖ Confusion matrix saved: /content/phase3_results/cm_task_a.png\n",
            "   ‚úÖ Confusion matrix saved: /content/phase3_results/cm_task_b.png\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "import torchvision.models as models\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, f1_score, accuracy_score, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from collections import Counter\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "\n",
        "# ============================================================================\n",
        "# CONFIGURATION\n",
        "# ============================================================================\n",
        "class Config:\n",
        "    # Paths\n",
        "    TASK_A_DATA_PATH = \"/content/data/OCT2017/train\"  # OCT images folder\n",
        "    TASK_B_DATA_PATH = \"/content/data/chest_xray/train\"  # Chest X-ray images folder\n",
        "    PHASE2_MODEL_PATH = \"/content/best_mobilenetv3_student_kd.pth\"\n",
        "    FISHER_PATH = \"/content/fisher/fisher_phase2.pth\"\n",
        "    SAVE_DIR = \"/content/phase3_results\"\n",
        "\n",
        "    # Model settings\n",
        "    TASK_A_CLASSES = 4  # OCT classes\n",
        "    TASK_B_CLASSES = 2  # Chest X-ray classes\n",
        "\n",
        "    # EWC hyperparameters\n",
        "    EWC_LAMBDA = 5000  # EWC regularization strength\n",
        "\n",
        "    # Training hyperparameters\n",
        "    BATCH_SIZE = 128\n",
        "    LEARNING_RATE = 0.0001\n",
        "    NUM_EPOCHS = 15\n",
        "    PATIENCE = 5  # Early stopping\n",
        "\n",
        "    # Data augmentation\n",
        "    USE_AUGMENTATION = True\n",
        "\n",
        "    # Evaluation\n",
        "    EVAL_TASK_A_EVERY = 2  # Evaluate Task A retention every N epochs\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# ============================================================================\n",
        "# DATA LOADING UTILITIES\n",
        "# ============================================================================\n",
        "def load_task_paths(data_path):\n",
        "    \"\"\"\n",
        "    Universal data loader for both OCT and Chest X-ray\n",
        "    Loads from: /path/to/train/CLASS_NAME/*.jpg\n",
        "    \"\"\"\n",
        "    data_path = Path(data_path)\n",
        "\n",
        "    # Get all class folders\n",
        "    class_names = sorted([d.name for d in data_path.iterdir() if d.is_dir()])\n",
        "\n",
        "    all_paths = []\n",
        "    all_labels = []\n",
        "\n",
        "    for idx, class_name in enumerate(class_names):\n",
        "        class_dir = data_path / class_name\n",
        "        # Support multiple image formats\n",
        "        paths = list(class_dir.glob('*.jpeg')) + \\\n",
        "                list(class_dir.glob('*.jpg')) + \\\n",
        "                list(class_dir.glob('*.png'))\n",
        "\n",
        "        all_paths.extend(paths)\n",
        "        all_labels.extend([idx] * len(paths))\n",
        "\n",
        "    return all_paths, all_labels, class_names\n",
        "\n",
        "# ============================================================================\n",
        "# DATASET CLASS\n",
        "# ============================================================================\n",
        "class ImageDataset(Dataset):\n",
        "    def __init__(self, paths, labels, transform=None):\n",
        "        self.paths = paths\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        from PIL import Image\n",
        "        img = Image.open(self.paths[idx]).convert('RGB')\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        return img, label\n",
        "\n",
        "# ============================================================================\n",
        "# DATA SPLITS\n",
        "# ============================================================================\n",
        "def create_task_a_splits():\n",
        "    \"\"\"Create stratified splits for Task A (OCT)\"\"\"\n",
        "    print(\"\\nüìä Creating Task A (OCT) evaluation splits...\")\n",
        "\n",
        "    all_paths, all_labels, task_a_class_names = load_task_paths(Config.TASK_A_DATA_PATH)\n",
        "    print(f\"   Total Task A samples: {len(all_paths):,}\")\n",
        "    print(f\"   Classes: {task_a_class_names}\")\n",
        "\n",
        "    # Class distribution\n",
        "    class_counts = Counter(all_labels)\n",
        "    print(f\"   Class distribution: {dict(class_counts)}\")\n",
        "\n",
        "    # 70/15/15 split\n",
        "    train_paths, temp_paths, train_labels, temp_labels = train_test_split(\n",
        "        all_paths, all_labels, test_size=0.30, stratify=all_labels, random_state=42\n",
        "    )\n",
        "\n",
        "    val_paths, test_paths, val_labels, test_labels = train_test_split(\n",
        "        temp_paths, temp_labels, test_size=0.50, stratify=temp_labels, random_state=42\n",
        "    )\n",
        "\n",
        "    print(f\"   Train: {len(train_paths):,} | Val: {len(val_paths):,} | Test: {len(test_paths):,}\")\n",
        "\n",
        "    test_transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    test_dataset = ImageDataset(test_paths, test_labels, test_transform)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=Config.BATCH_SIZE,\n",
        "                            shuffle=False, num_workers=2)\n",
        "\n",
        "    return test_loader, task_a_class_names\n",
        "\n",
        "def create_task_b_splits():\n",
        "    \"\"\"Create stratified splits for Task B (Chest X-ray)\"\"\"\n",
        "    print(\"\\nüìÇ Creating Task B (Chest X-ray) splits...\")\n",
        "\n",
        "    all_paths, all_labels, task_b_class_names = load_task_paths(Config.TASK_B_DATA_PATH)\n",
        "    print(f\"   Total samples: {len(all_paths):,}\")\n",
        "    print(f\"   Classes: {task_b_class_names}\")\n",
        "\n",
        "    # Class distribution\n",
        "    class_counts = Counter(all_labels)\n",
        "    print(f\"   Class distribution: {dict(class_counts)}\")\n",
        "\n",
        "    # 70/15/15 stratified split\n",
        "    train_paths, temp_paths, train_labels, temp_labels = train_test_split(\n",
        "        all_paths, all_labels, test_size=0.30, stratify=all_labels, random_state=42\n",
        "    )\n",
        "\n",
        "    val_paths, test_paths, val_labels, test_labels = train_test_split(\n",
        "        temp_paths, temp_labels, test_size=0.50, stratify=temp_labels, random_state=42\n",
        "    )\n",
        "\n",
        "    print(f\"   Train: {len(train_paths):,} | Val: {len(val_paths):,} | Test: {len(test_paths):,}\")\n",
        "\n",
        "    # Compute class weights for imbalanced dataset\n",
        "    train_class_counts = Counter(train_labels)\n",
        "    total_samples = len(train_labels)\n",
        "    class_weights = torch.tensor([\n",
        "        total_samples / (len(train_class_counts) * train_class_counts[i])\n",
        "        for i in range(len(task_b_class_names))\n",
        "    ], dtype=torch.float32).to(Config.device)\n",
        "\n",
        "    print(f\"   Class weights: {class_weights.cpu().numpy()}\")\n",
        "\n",
        "    # Data transforms with augmentation\n",
        "    if Config.USE_AUGMENTATION:\n",
        "        train_transform = transforms.Compose([\n",
        "            transforms.Resize((224, 224)),\n",
        "            transforms.RandomHorizontalFlip(p=0.5),\n",
        "            transforms.RandomRotation(10),\n",
        "            transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "        ])\n",
        "    else:\n",
        "        train_transform = transforms.Compose([\n",
        "            transforms.Resize((224, 224)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "        ])\n",
        "\n",
        "    val_transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    train_dataset = ImageDataset(train_paths, train_labels, train_transform)\n",
        "    val_dataset = ImageDataset(val_paths, val_labels, val_transform)\n",
        "    test_dataset = ImageDataset(test_paths, test_labels, val_transform)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=Config.BATCH_SIZE,\n",
        "                             shuffle=True, num_workers=2)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=Config.BATCH_SIZE,\n",
        "                           shuffle=False, num_workers=2)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=Config.BATCH_SIZE,\n",
        "                            shuffle=False, num_workers=2)\n",
        "\n",
        "    return train_loader, val_loader, test_loader, class_weights, task_b_class_names\n",
        "\n",
        "# ============================================================================\n",
        "# MULTI-HEAD MODEL\n",
        "# ============================================================================\n",
        "class MultiHeadMobileNet(nn.Module):\n",
        "    def __init__(self, num_classes_a, num_classes_b):\n",
        "        super().__init__()\n",
        "        # Load MobileNetV3\n",
        "        mobilenet = models.mobilenet_v3_large(weights=None)\n",
        "        self.features = mobilenet.features  # Backbone features\n",
        "\n",
        "        # ADD THIS - from Phase 2\n",
        "        self.feature_projector = nn.Sequential(\n",
        "            nn.Linear(960, 1024),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(1024, 2048)\n",
        "        )\n",
        "\n",
        "        # Task A head (OCT)\n",
        "        self.head_a = nn.Sequential(\n",
        "            nn.Linear(960, 256),\n",
        "            nn.Hardswish(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(256, num_classes_a)\n",
        "        )\n",
        "\n",
        "        # Task B head (Chest X-ray) - new\n",
        "        self.head_b = nn.Sequential(\n",
        "            nn.Linear(960, 256),\n",
        "            nn.Hardswish(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(256, num_classes_b)\n",
        "        )\n",
        "\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.flatten = nn.Flatten()\n",
        "\n",
        "    def forward(self, x, task='b'):\n",
        "        x = self.features(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = self.flatten(x)\n",
        "\n",
        "        if task == 'a':\n",
        "            return self.head_a(x)\n",
        "        elif task == 'b':\n",
        "            return self.head_b(x)\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown task: {task}\")\n",
        "\n",
        "# ============================================================================\n",
        "# EWC LOSS\n",
        "# ============================================================================\n",
        "\n",
        "def compute_ewc_loss(model, fisher_dict, optimal_params, lambda_ewc):\n",
        "    ewc_loss = 0.0\n",
        "\n",
        "    for name, param in model.named_parameters():\n",
        "        if name in fisher_dict:  \n",
        "            fisher = fisher_dict[name].to(param.device)  # Device safety\n",
        "            optimal = optimal_params[name].to(param.device)  # Device safety\n",
        "            ewc_loss += (fisher * (param - optimal).pow(2)).sum()\n",
        "\n",
        "    return (lambda_ewc / 2.0) * ewc_loss\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# LOAD PHASE 2 MODEL AND FISHER\n",
        "# ============================================================================\n",
        "def load_phase2_assets():\n",
        "    \"\"\"Load Phase 2 model and Fisher information\"\"\"\n",
        "    print(\"\\n Loading Phase 2 assets...\")\n",
        "\n",
        "    # Load Fisher data (contains Phase 2 model weights)\n",
        "    fisher_data = torch.load(Config.FISHER_PATH, map_location=Config.device)\n",
        "    phase2_state = fisher_data['model_state_dict']\n",
        "\n",
        "    # Create multi-head model\n",
        "    model = MultiHeadMobileNet(Config.TASK_A_CLASSES, Config.TASK_B_CLASSES)\n",
        "\n",
        "    # Load weights with correct mapping\n",
        "    model_state = {}\n",
        "\n",
        "    for key, value in phase2_state.items():\n",
        "        if key.startswith('backbone.features'):\n",
        "            # backbone.features.X -> features.X\n",
        "            new_key = key.replace('backbone.', '')\n",
        "            model_state[new_key] = value\n",
        "        elif key.startswith('backbone.classifier'):\n",
        "            # backbone.classifier.X -> head_a.X\n",
        "            new_key = key.replace('backbone.classifier', 'head_a')\n",
        "            model_state[new_key] = value\n",
        "        elif key.startswith('feature_projector'):\n",
        "            # Keep feature_projector as-is\n",
        "            model_state[key] = value\n",
        "\n",
        "    # Load the mapped weights\n",
        "    model.load_state_dict(model_state, strict=False)\n",
        "    model = model.to(Config.device)\n",
        "    print(\"   ‚úÖ Phase 2 model loaded\")\n",
        "\n",
        "    # Load Fisher information\n",
        "    fisher_dict = fisher_data['fisher_information']\n",
        "    optimal_params = phase2_state\n",
        "\n",
        "    # Map Fisher keys to new model structure\n",
        "    # ‚úÖ IMPORTANT: Only iterate over Fisher keys (trainable params only)\n",
        "    mapped_fisher = {}\n",
        "    mapped_optimal = {}\n",
        "\n",
        "    for key in fisher_dict.keys():  # Fisher only has trainable parameters\n",
        "        if key.startswith('backbone.features'):\n",
        "            # backbone.features.X -> features.X\n",
        "            new_key = key.replace('backbone.', '')\n",
        "        elif key.startswith('backbone.classifier'):\n",
        "            # backbone.classifier.X -> head_a.X\n",
        "            new_key = key.replace('backbone.classifier', 'head_a')\n",
        "        elif key.startswith('feature_projector'):\n",
        "            # Keep feature_projector as-is\n",
        "            new_key = key\n",
        "        else:\n",
        "            continue\n",
        "\n",
        "        # Map both Fisher and optimal params\n",
        "        mapped_fisher[new_key] = fisher_dict[key]\n",
        "        mapped_optimal[new_key] = optimal_params[key]\n",
        "\n",
        "    print(f\"   ‚úÖ Fisher information loaded ({len(mapped_fisher)} parameters)\")\n",
        "    print(f\"   ‚úÖ Optimal parameters loaded ({len(mapped_optimal)} parameters)\")\n",
        "\n",
        "    # Verification\n",
        "    print(f\"\\n   üîç Model Structure Verification:\")\n",
        "    print(f\"      features: {sum(p.numel() for p in model.features.parameters()):,} params\")\n",
        "    print(f\"      head_a: {sum(p.numel() for p in model.head_a.parameters()):,} params\")\n",
        "    print(f\"      head_b: {sum(p.numel() for p in model.head_b.parameters()):,} params\")\n",
        "    print(f\"      feature_projector: {sum(p.numel() for p in model.feature_projector.parameters()):,} params\")\n",
        "\n",
        "    return model, mapped_fisher, mapped_optimal\n",
        "\n",
        "# ============================================================================\n",
        "# EVALUATION FUNCTIONS\n",
        "# ============================================================================\n",
        "def evaluate_task(model, dataloader, task, class_names):\n",
        "    \"\"\"Evaluate model on a specific task\"\"\"\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in dataloader:\n",
        "            images = images.to(Config.device)\n",
        "            outputs = model(images, task=task)\n",
        "            preds = outputs.argmax(dim=1)\n",
        "\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.numpy())\n",
        "\n",
        "    acc = accuracy_score(all_labels, all_preds)\n",
        "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
        "\n",
        "    return acc, f1, all_preds, all_labels\n",
        "\n",
        "def print_evaluation_report(acc, f1, preds, labels, class_names, task_name):\n",
        "    \"\"\"Print detailed evaluation report\"\"\"\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"üìä {task_name} EVALUATION\")\n",
        "    print(f\"{'='*70}\")\n",
        "    print(f\"   Accuracy:  {acc*100:.2f}%\")\n",
        "    print(f\"   F1-Score:  {f1:.4f}\")\n",
        "    print(f\"\\nüìã Classification Report:\")\n",
        "    print(classification_report(labels, preds, target_names=class_names, digits=4))\n",
        "\n",
        "# ============================================================================\n",
        "# TRAINING FUNCTION\n",
        "# ============================================================================\n",
        "def train_phase3():\n",
        "    \"\"\"Phase 3: Continual Learning with EWC\"\"\"\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"üöÄ PHASE 3: CONTINUAL LEARNING WITH EWC\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # Create save directory\n",
        "    Path(Config.SAVE_DIR).mkdir(exist_ok=True)\n",
        "\n",
        "    # Load Phase 2 assets\n",
        "    model, fisher_dict, optimal_params = load_phase2_assets()\n",
        "\n",
        "\n",
        "            # Freeze Task A head\n",
        "    for p in model.head_a.parameters():\n",
        "        p.requires_grad = False\n",
        "\n",
        "    # verify\n",
        "    for name, p in model.named_parameters():\n",
        "        if \"head_a\" in name:\n",
        "            assert p.requires_grad is False\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # Create Task A test loader for retention evaluation\n",
        "    task_a_test_loader, task_a_classes = create_task_a_splits()\n",
        "\n",
        "    # Evaluate Task A before fine-tuning (baseline)\n",
        "    print(\"\\nüß™ Evaluating Task A (OCT) BEFORE fine-tuning...\")\n",
        "    task_a_acc_before, task_a_f1_before, _, _ = evaluate_task(\n",
        "        model, task_a_test_loader, task='a', class_names=task_a_classes\n",
        "    )\n",
        "    print(f\"   Task A Accuracy: {task_a_acc_before*100:.2f}%\")\n",
        "    print(f\"   Task A F1: {task_a_f1_before:.4f}\")\n",
        "\n",
        "    # Create Task B dataloaders\n",
        "    train_loader, val_loader, test_loader, class_weights, task_b_classes = create_task_b_splits()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # Setup training\n",
        "    # In train_phase3 function:\n",
        "    optimizer = optim.Adam([\n",
        "      {'params': model.features.parameters(), 'lr': Config.LEARNING_RATE},\n",
        "      {'params': model.head_b.parameters(), 'lr': Config.LEARNING_RATE}\n",
        "    ])\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5,\n",
        "                                                     patience=3,)\n",
        "\n",
        "    # Training loop\n",
        "    best_val_f1 = 0.0\n",
        "    patience_counter = 0\n",
        "    history = {'train_loss': [], 'val_f1': [], 'task_a_f1': []}\n",
        "\n",
        "    print(f\"\\nüéØ Training Task B (Chest X-ray) with EWC (Œª={Config.EWC_LAMBDA})...\")\n",
        "\n",
        "    for epoch in range(Config.NUM_EPOCHS):\n",
        "        # Training\n",
        "        model.train()\n",
        "        model.features.eval()  # Freeze backbone BatchNorm\n",
        "        #model.head_a.eval() # Freeze head_a\n",
        "        train_loss = 0.0\n",
        "\n",
        "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{Config.NUM_EPOCHS}\")\n",
        "        for images, labels in pbar:\n",
        "            images, labels = images.to(Config.device), labels.to(Config.device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images, task='b')\n",
        "\n",
        "            # Task B loss (cross-entropy with class weights)\n",
        "            ce_loss = criterion(outputs, labels)\n",
        "\n",
        "            # EWC regularization loss\n",
        "            ewc_loss = compute_ewc_loss(model, fisher_dict, optimal_params, Config.EWC_LAMBDA)\n",
        "\n",
        "            # Total loss\n",
        "            total_loss = ce_loss + ewc_loss\n",
        "\n",
        "            total_loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += total_loss.item()\n",
        "            pbar.set_postfix({'loss': f'{total_loss.item():.4f}',\n",
        "                            'ce': f'{ce_loss.item():.4f}',\n",
        "                            'ewc': f'{ewc_loss.item():.4f}'})\n",
        "\n",
        "        avg_train_loss = train_loss / len(train_loader)\n",
        "        history['train_loss'].append(avg_train_loss)\n",
        "\n",
        "        # Validation on Task B\n",
        "        val_acc, val_f1, _, _ = evaluate_task(model, val_loader, task='b',\n",
        "                                             class_names=task_b_classes)\n",
        "        history['val_f1'].append(val_f1)\n",
        "\n",
        "        print(f\"\\n   Epoch {epoch+1} - Task B Val F1: {val_f1:.4f} | Acc: {val_acc*100:.2f}%\")\n",
        "\n",
        "        # Evaluate Task A retention periodically\n",
        "        if (epoch + 1) % Config.EVAL_TASK_A_EVERY == 0:\n",
        "            task_a_acc, task_a_f1, _, _ = evaluate_task(model, task_a_test_loader,\n",
        "                                                        task='a', class_names=task_a_classes)\n",
        "            history['task_a_f1'].append(task_a_f1)\n",
        "            retention = (task_a_f1 / task_a_f1_before) * 100\n",
        "            print(f\"   üìà Task A Retention: F1={task_a_f1:.4f} ({retention:.2f}% of baseline)\")\n",
        "\n",
        "        # Learning rate scheduling\n",
        "        scheduler.step(val_f1)\n",
        "\n",
        "        # Early stopping and checkpointing\n",
        "        if val_f1 > best_val_f1:\n",
        "            best_val_f1 = val_f1\n",
        "            patience_counter = 0\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'val_f1': val_f1,\n",
        "                'task_a_f1_before': task_a_f1_before\n",
        "            }, f\"{Config.SAVE_DIR}/phase3_best.pth\")\n",
        "            print(f\"   üíæ Best model saved (Val F1: {val_f1:.4f})\")\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= Config.PATIENCE:\n",
        "                print(f\"\\n‚è∏Ô∏è  Early stopping triggered (patience={Config.PATIENCE})\")\n",
        "                break\n",
        "\n",
        "    # Final evaluation\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"üìä FINAL EVALUATION\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # Load best model\n",
        "    checkpoint = torch.load(f\"{Config.SAVE_DIR}/phase3_best.pth\")\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "    # Task B (Chest X-ray) - Test set\n",
        "    task_b_acc, task_b_f1, task_b_preds, task_b_labels = evaluate_task(\n",
        "        model, test_loader, task='b', class_names=task_b_classes\n",
        "    )\n",
        "    print_evaluation_report(task_b_acc, task_b_f1, task_b_preds, task_b_labels,\n",
        "                          task_b_classes, \"TASK B (Chest X-ray)\")\n",
        "\n",
        "    # Task A (OCT) - Retention test\n",
        "    task_a_acc_after, task_a_f1_after, task_a_preds, task_a_labels = evaluate_task(\n",
        "        model, task_a_test_loader, task='a', class_names=task_a_classes\n",
        "    )\n",
        "    print_evaluation_report(task_a_acc_after, task_a_f1_after, task_a_preds, task_a_labels,\n",
        "                          task_a_classes, \"TASK A (OCT) - Retention Check\")\n",
        "\n",
        "    # Retention metrics\n",
        "    retention_f1 = (task_a_f1_after / task_a_f1_before) * 100\n",
        "    retention_acc = (task_a_acc_after / task_a_acc_before) * 100\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"üéØ CONTINUAL LEARNING SUMMARY\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"üìä Task A (OCT) Retention:\")\n",
        "    print(f\"   Before: F1={task_a_f1_before:.4f}, Acc={task_a_acc_before*100:.2f}%\")\n",
        "    print(f\"   After:  F1={task_a_f1_after:.4f}, Acc={task_a_acc_after*100:.2f}%\")\n",
        "    print(f\"   Retention: F1={retention_f1:.2f}%, Acc={retention_acc:.2f}%\")\n",
        "    print(f\"\\nüìä Task B (Chest X-ray) Performance:\")\n",
        "    print(f\"   Test F1: {task_b_f1:.4f}\")\n",
        "    print(f\"   Test Acc: {task_b_acc*100:.2f}%\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # Save confusion matrices\n",
        "    save_confusion_matrix(task_a_labels, task_a_preds, task_a_classes,\n",
        "                         \"Task A (OCT) - After EWC\", f\"{Config.SAVE_DIR}/cm_task_a.png\")\n",
        "    save_confusion_matrix(task_b_labels, task_b_preds, task_b_classes,\n",
        "                         \"Task B (Chest X-ray)\", f\"{Config.SAVE_DIR}/cm_task_b.png\")\n",
        "\n",
        "    return model, history\n",
        "\n",
        "def save_confusion_matrix(labels, preds, class_names, title, save_path):\n",
        "    \"\"\"Save confusion matrix plot\"\"\"\n",
        "    cm = confusion_matrix(labels, preds)\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=class_names, yticklabels=class_names)\n",
        "    plt.title(title)\n",
        "    plt.ylabel('True Label')\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "    print(f\"   ‚úÖ Confusion matrix saved: {save_path}\")\n",
        "\n",
        "# ============================================================================\n",
        "# MAIN EXECUTION\n",
        "# ============================================================================\n",
        "if __name__ == \"__main__\":\n",
        "    model, history = train_phase3()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "04695279e8b246e2abaf60c318692906": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "28bafff2b60946628901c8e9337df4db": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3ffcdc72f0dc4a53854f457bf29032dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "44e8db6832e84f38835e08e328897e53": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4da2eb8b56de47eba1977f1f58b36c3f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "72e461b069c2465a8ae936de4c99b53f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_dc4f36b5c6a146be90016122248ba3a9",
              "IPY_MODEL_82cc9533a8964f7fa7adc9e0cc7d4067",
              "IPY_MODEL_e50463f3086d44b0810eb43cd4d6eb16"
            ],
            "layout": "IPY_MODEL_4da2eb8b56de47eba1977f1f58b36c3f"
          }
        },
        "82cc9533a8964f7fa7adc9e0cc7d4067": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_44e8db6832e84f38835e08e328897e53",
            "max": 229,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3ffcdc72f0dc4a53854f457bf29032dc",
            "value": 229
          }
        },
        "b1b3c59a38064340a461bf7876472cc6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "be201640562d41baba14a7b932edf906": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dc4f36b5c6a146be90016122248ba3a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b1b3c59a38064340a461bf7876472cc6",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_be201640562d41baba14a7b932edf906",
            "value": "Computing‚ÄáFisher:‚Äá100%"
          }
        },
        "e50463f3086d44b0810eb43cd4d6eb16": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_04695279e8b246e2abaf60c318692906",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_28bafff2b60946628901c8e9337df4db",
            "value": "‚Äá229/229‚Äá[06:38&lt;00:00,‚Äá‚Äá1.12s/it,‚Äásamples=58438]"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
